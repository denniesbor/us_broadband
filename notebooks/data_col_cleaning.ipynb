{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93a60c32-3041-4583-8428-e4dc480de4b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os\n",
    "from io import StringIO\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from multiprocessing import Process, current_process, Pool\n",
    "import gc\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "194a8fb7-ec1d-4537-9189-4796e38c3bf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# configure data path\n",
    "\n",
    "current_path = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "data_path = os.path.join(current_path, \"..\", \"data\")\n",
    "\n",
    "data_path = os.path.normpath(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66370d40-66f2-4bbb-b342-fea6e1db9a6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scrape_pages(loc, link):\n",
    "    download_loc = os.path.join(data_path, loc)\n",
    "\n",
    "    # check if location exists\n",
    "    if not os.path.exists(download_loc):\n",
    "        # if the directory does not exist, create it\n",
    "        os.makedirs(download_loc)\n",
    "\n",
    "    # configure selenium and download path\n",
    "    options = Options()\n",
    "    options.set_preference(\"browser.download.folderList\", 2)\n",
    "    options.set_preference(\"browser.download.manager.showWhenStarting\", False)\n",
    "    options.set_preference(\"browser.download.dir\", download_loc)\n",
    "\n",
    "    # get the state page with requests\n",
    "    html_content = requests.get(link)\n",
    "\n",
    "    # beautify the page\n",
    "    soup = BeautifulSoup(html_content.text, \"html.parser\")\n",
    "\n",
    "    # get tables in the hrml page with links to state broadband data\n",
    "    tables = soup.find_all(\"table\")[1]\n",
    "    links = tables.find_all(\"a\")\n",
    "\n",
    "    # loop through the links and download the files using selenium\n",
    "    for link in links:\n",
    "        href = link.get(\"href\")\n",
    "\n",
    "        download_filename = href.split(\"/\")[-1]\n",
    "        download_path = os.path.join(download_loc, download_filename)\n",
    "        temp_download_path = download_path + \".part\"\n",
    "\n",
    "        # check if download files exist\n",
    "        if os.path.isfile(download_path):\n",
    "            print(f\"{download_filename} exist\")\n",
    "        else:\n",
    "            # selenium driver\n",
    "            driver = webdriver.Firefox(options=options)\n",
    "            driver.set_page_load_timeout(5)\n",
    "            try:\n",
    "                driver.get(href)\n",
    "            except TimeoutException:\n",
    "                print(\"Page load timed out but continuing execution.\")\n",
    "\n",
    "            # wait for the download to complete, checking every second\n",
    "            while True:\n",
    "                files = os.listdir(download_loc)\n",
    "                # Check if the .part file for the current download exists\n",
    "                if any(\n",
    "                    file.startswith(download_filename.split(\".\")[0])\n",
    "                    and file.endswith(\".part\")\n",
    "                    for file in files\n",
    "                ):\n",
    "                    print(f\"Download in progress for {download_filename}...\")\n",
    "                    time.sleep(1)\n",
    "                else:\n",
    "                    # If there's no .part file, the download is complete\n",
    "                    print(f\"Download completed for {download_filename}.\")\n",
    "                    break\n",
    "\n",
    "            driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb5aa835-a0ea-47d6-8264-3312a55ac219",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AK-Fixed-Dec2017.zip existAK-Fixed-Dec2018.zip exist\n",
      "\n",
      "AL-Fixed-Dec2018.zip existAL-Fixed-Dec2017.zip exist\n",
      "\n",
      "AR-Fixed-Dec2018.zip existAR-Fixed-Dec2017.zip exist\n",
      "\n",
      "AS-Fixed-Dec2018.zip existAS-Fixed-Dec2017.zip exist\n",
      "\n",
      "AZ-Fixed-Dec2018.zip existAZ-Fixed-Dec2017.zip exist\n",
      "\n",
      "CA-Fixed-Dec2017.zip existCA-Fixed-Dec2018.zip exist\n",
      "\n",
      "CO-Fixed-Dec2017.zip existCO-Fixed-Dec2018.zip exist\n",
      "\n",
      "CT-Fixed-Dec2017.zip existCT-Fixed-Dec2018.zip exist\n",
      "\n",
      "DC-Fixed-Dec2018.zip existDC-Fixed-Dec2017.zip exist\n",
      "\n",
      "DE-Fixed-Dec2018.zip existDE-Fixed-Dec2017.zip exist\n",
      "\n",
      "FL-Fixed-Dec2018.zip existFL-Fixed-Dec2017.zip exist\n",
      "\n",
      "GA-Fixed-Dec2018.zip existGA-Fixed-Dec2017.zip exist\n",
      "\n",
      "GU-Fixed-Dec2018.zip existGU-Fixed-Dec2017.zip exist\n",
      "\n",
      "HI-Fixed-Dec2017.zip existHI-Fixed-Dec2018.zip exist\n",
      "\n",
      "IA-Fixed-Dec2017.zip existIA-Fixed-Dec2018.zip exist\n",
      "\n",
      "ID-Fixed-Dec2017.zip exist\n",
      "ID-Fixed-Dec2018.zip existIL-Fixed-Dec2017.zip exist\n",
      "\n",
      "IL-Fixed-Dec2018.zip existIN-Fixed-Dec2017.zip exist\n",
      "\n",
      "IN-Fixed-Dec2018.zip existKS-Fixed-Dec2017.zip exist\n",
      "\n",
      "KS-Fixed-Dec2018.zip existKY-Fixed-Dec2017.zip exist\n",
      "\n",
      "KY-Fixed-Dec2018.zip existLA-Fixed-Dec2017.zip exist\n",
      "\n",
      "LA-Fixed-Dec2018.zip existMA-Fixed-Dec2017.zip exist\n",
      "\n",
      "MA-Fixed-Dec2018.zip existMD-Fixed-Dec2017.zip exist\n",
      "\n",
      "MD-Fixed-Dec2018.zip existME-Fixed-Dec2017.zip exist\n",
      "\n",
      "ME-Fixed-Dec2018.zip existMI-Fixed-Dec2017.zip exist\n",
      "\n",
      "MI-Fixed-Dec2018.zip existMN-Fixed-Dec2017.zip exist\n",
      "\n",
      "MN-Fixed-Dec2018.zip existMO-Fixed-Dec2017.zip exist\n",
      "\n",
      "MO-Fixed-Dec2018.zip existMP-Fixed-Dec2017.zip exist\n",
      "\n",
      "MP-Fixed-Dec2018.zip existMS-Fixed-Dec2017.zip exist\n",
      "\n",
      "MS-Fixed-Dec2018.zip existMT-Fixed-Dec2017.zip exist\n",
      "\n",
      "NC-Fixed-Dec2017.zip existMT-Fixed-Dec2018.zip exist\n",
      "\n",
      "ND-Fixed-Dec2017.zip existNC-Fixed-Dec2018.zip exist\n",
      "\n",
      "ND-Fixed-Dec2018.zip existNE-Fixed-Dec2017.zip exist\n",
      "\n",
      "NH-Fixed-Dec2017.zip existNE-Fixed-Dec2018.zip exist\n",
      "\n",
      "NJ-Fixed-Dec2017.zip exist\n",
      "NH-Fixed-Dec2018.zip existNM-Fixed-Dec2017.zip exist\n",
      "\n",
      "NV-Fixed-Dec2017.zip existNJ-Fixed-Dec2018.zip exist\n",
      "\n",
      "NY-Fixed-Dec2017.zip exist\n",
      "NM-Fixed-Dec2018.zip existOH-Fixed-Dec2017.zip exist\n",
      "\n",
      "OK-Fixed-Dec2017.zip exist\n",
      "NV-Fixed-Dec2018.zip existOR-Fixed-Dec2017.zip exist\n",
      "\n",
      "PA-Fixed-Dec2017.zip existNY-Fixed-Dec2018.zip exist\n",
      "PR-Fixed-Dec2017.zip exist\n",
      "\n",
      "RI-Fixed-Dec2017.zip existOH-Fixed-Dec2018.zip exist\n",
      "\n",
      "SC-Fixed-Dec2017.zip exist\n",
      "OK-Fixed-Dec2018.zip existSD-Fixed-Dec2017.zip exist\n",
      "\n",
      "OR-Fixed-Dec2018.zip existTN-Fixed-Dec2017.zip exist\n",
      "\n",
      "TX-Fixed-Dec2017.zip exist\n",
      "PA-Fixed-Dec2018.zip existUT-Fixed-Dec2017.zip exist\n",
      "\n",
      "VA-Fixed-Dec2017.zip existPR-Fixed-Dec2018.zip exist\n",
      "\n",
      "VI-Fixed-Dec2017.zip exist\n",
      "RI-Fixed-Dec2018.zip existVT-Fixed-Dec2017.zip exist\n",
      "\n",
      "WA-Fixed-Dec2017.zip existSC-Fixed-Dec2018.zip exist\n",
      "\n",
      "WI-Fixed-Dec2017.zip existSD-Fixed-Dec2018.zip exist\n",
      "\n",
      "WV-Fixed-Dec2017.zip existTN-Fixed-Dec2018.zip exist\n",
      "\n",
      "TX-Fixed-Dec2018.zip existWY-Fixed-Dec2017.zip exist\n",
      "UT-Fixed-Dec2018.zip exist\n",
      "\n",
      "VA-Fixed-Dec2018.zip exist\n",
      "VI-Fixed-Dec2018.zip exist\n",
      "VT-Fixed-Dec2018.zip exist\n",
      "WA-Fixed-Dec2018.zip exist\n",
      "WI-Fixed-Dec2018.zip exist\n",
      "WV-Fixed-Dec2018.zip existWY-Fixed-Dec2018.zip exist\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using multiprocessor\n",
    "def scrape_wrapper(args):\n",
    "    return scrape_pages(*args)\n",
    "\n",
    "\n",
    "# List of arguments for each dataset\n",
    "datasets = [\n",
    "    (\n",
    "        \"2017\",\n",
    "        \"https://www.fcc.gov/form-477-broadband-deployment-data-december-2017-version-3\",\n",
    "    ),\n",
    "    (\n",
    "        \"2018\",\n",
    "        \"https://www.fcc.gov/form-477-broadband-deployment-data-december-2018-version-3\",\n",
    "    ),\n",
    "    # (\"2020\", \"https://www.fcc.gov/form-477-broadband-deployment-data-december-2020\"),\n",
    "]\n",
    "\n",
    "# Using Pool to create a pool of worker processes\n",
    "with Pool(processes=len(datasets)) as pool:\n",
    "    # map the datasets to the scrape_wrapper function\n",
    "    pool.map(scrape_wrapper, datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc7da47e-57ab-4cdf-8e57-75e2a675a719",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# break the 2020, 2021, and 2019 large fcc deployment datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "192c7e92-0533-4a93-be63-a23121d57dc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chunk_size = 200000\n",
    "\n",
    "# latest technologies\n",
    "latest_tech_codes = [43, 50]\n",
    "\n",
    "\n",
    "def break_csv(year, file_name):\n",
    "    csv_path = os.path.join(data_path, file_name)\n",
    "    file_path = os.path.join(data_path, year)\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        os.makedirs(file_path)\n",
    "\n",
    "    # use an iterator to read in chunks\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(\n",
    "            os.path.join(data_path, \"fbd_us_without_satellite_dec2019_v1.csv\"),\n",
    "            chunksize=chunk_size,\n",
    "            encoding=\"utf8\",\n",
    "            encoding_errors=\"ignore\",\n",
    "        )\n",
    "    ):\n",
    "        # Process each chunk\n",
    "        for state_abbr, group_df in chunk.groupby(\"StateAbbr\"):\n",
    "            # Define the filename for each state's CSV\n",
    "            filename = f\"{state_abbr}_{year}.csv\"\n",
    "\n",
    "            # latest tech\n",
    "            group_df[\"IsLatestTech\"] = group_df[\"TechCode\"].isin(latest_tech_codes)\n",
    "\n",
    "            # consumer dataframes\n",
    "            consumer_df = group_df[group_df[\"Consumer\"] == 1]\n",
    "\n",
    "            if consumer_df.shape[0] == 0:\n",
    "                continue\n",
    "            # Group by 'BlockCode' and calculate the median 'MaxAdDown' and 'MaxAdUp'\n",
    "            block_speeds = (\n",
    "                consumer_df.groupby(\"BlockCode\")[[\"MaxAdDown\", \"MaxAdUp\"]]\n",
    "                .median()\n",
    "                .reset_index()\n",
    "            )\n",
    "\n",
    "            tech_ratio = (\n",
    "                consumer_df.groupby(\"BlockCode\")\n",
    "                .apply(lambda x: x[\"IsLatestTech\"].sum() / len(x))\n",
    "                .reset_index(name=\"LTRatio\")\n",
    "            )\n",
    "\n",
    "            df_ = block_speeds.merge(tech_ratio, on=\"BlockCode\")\n",
    "\n",
    "            df_.rename(\n",
    "                columns={\n",
    "                    \"MaxAdDown\": \"MaxAdDown\" + year,\n",
    "                    \"MaxAdUp\": \"MaxAdUp\" + year,\n",
    "                    \"LTRatio\": \"LTRatio\" + year,\n",
    "                },\n",
    "                inplace=True,\n",
    "            )\n",
    "\n",
    "            # Append data to the CSV if it already exists, else create a new one\n",
    "            with open(os.path.join(file_path, filename), \"a\") as f:\n",
    "                df_.to_csv(f, index=False, header=f.tell() == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e6fd90a-7a92-4c06-88c1-9b28cdc63da8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def multiprocessor_wrapper(args):\n",
    "    return break_csv(*args)\n",
    "\n",
    "\n",
    "data = [\n",
    "    (\"2019\", \"fbd_us_without_satellite_dec2019_v1.csv\"),\n",
    "    (\"2020\", \"fbd_us_without_satellite_dec2020_v1.csv\"),\n",
    "    (\"2021\", \"fbd_us_without_satellite_dec2021_v1.csv\"),\n",
    "]\n",
    "\n",
    "# with Pool(processes=len(data)) as pool:\n",
    "#     # map the datasets to the scrape_wrapper function\n",
    "#     pool.map(multiprocessor_wrapper, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a208401-5e31-4e93-be00-57a78270c576",
   "metadata": {},
   "source": [
    "### Download 2020 and 2010 census block relationship files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1c260b0-f1e3-4e3f-9263-45ed846ddba6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# download the 2020 2010 block codes concordances\n",
    "\n",
    "# get the state page with requests\n",
    "html_content = requests.get(\n",
    "    \"https://www.census.gov/geographies/reference-files/time-series/geo/relationship-files.2020.html#blkgrp\"\n",
    ")\n",
    "\n",
    "# beautify the page\n",
    "soup = BeautifulSoup(html_content.text, \"html.parser\")\n",
    "\n",
    "# # get tables in the hrml page with links to state broadband data\n",
    "# tables = soup.find_all('table')[1]\n",
    "# links = tables.find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c9c453d-a25e-49e0-bbb6-ee37f62bdb91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TAB2010_TAB2020_ST04.zip exists\n",
      "TAB2010_TAB2020_ST02.zip exists\n",
      "TAB2010_TAB2020_ST06.zip exists\n",
      "TAB2010_TAB2020_ST01.zip exists\n",
      "TAB2010_TAB2020_ST05.zip exists\n",
      "TAB2010_TAB2020_ST08.zip exists\n",
      "TAB2010_TAB2020_ST09.zip exists\n",
      "TAB2010_TAB2020_ST10.zip exists\n",
      "TAB2010_TAB2020_ST16.zip exists\n",
      "TAB2010_TAB2020_ST13.zip exists\n",
      "TAB2010_TAB2020_ST15.zip exists\n",
      "TAB2010_TAB2020_ST17.zip exists\n",
      "TAB2010_TAB2020_ST12.zip exists\n",
      "TAB2010_TAB2020_ST11.zip exists\n",
      "TAB2010_TAB2020_ST18.zip exists\n",
      "TAB2010_TAB2020_ST21.zip exists\n",
      "TAB2010_TAB2020_ST19.zip exists\n",
      "TAB2010_TAB2020_ST20.zip exists\n",
      "TAB2010_TAB2020_ST22.zip exists\n",
      "TAB2010_TAB2020_ST23.zip exists\n",
      "TAB2010_TAB2020_ST24.zip exists\n",
      "TAB2010_TAB2020_ST25.zip exists\n",
      "TAB2010_TAB2020_ST26.zip exists\n",
      "TAB2010_TAB2020_ST28.zip exists\n",
      "TAB2010_TAB2020_ST27.zip exists\n",
      "TAB2010_TAB2020_ST30.zip exists\n",
      "TAB2010_TAB2020_ST29.zip exists\n",
      "TAB2010_TAB2020_ST31.zip exists\n",
      "TAB2010_TAB2020_ST35.zip exists\n",
      "TAB2010_TAB2020_ST32.zip exists\n",
      "TAB2010_TAB2020_ST33.zip exists\n",
      "TAB2010_TAB2020_ST37.zip exists\n",
      "TAB2010_TAB2020_ST40.zip exists\n",
      "TAB2010_TAB2020_ST39.zip exists\n",
      "TAB2010_TAB2020_ST34.zip exists\n",
      "TAB2010_TAB2020_ST38.zip exists\n",
      "TAB2010_TAB2020_ST42.zip exists\n",
      "TAB2010_TAB2020_ST36.zip exists\n",
      "TAB2010_TAB2020_ST44.zip exists\n",
      "TAB2010_TAB2020_ST41.zip exists\n",
      "TAB2010_TAB2020_ST45.zip exists\n",
      "TAB2010_TAB2020_ST47.zip exists\n",
      "TAB2010_TAB2020_ST46.zip exists\n",
      "TAB2010_TAB2020_ST50.zip exists\n",
      "TAB2010_TAB2020_ST48.zip exists\n",
      "TAB2010_TAB2020_ST53.zip exists\n",
      "TAB2010_TAB2020_ST51.zip exists\n",
      "TAB2010_TAB2020_ST54.zip exists\n",
      "TAB2010_TAB2020_ST49.zip exists\n",
      "TAB2010_TAB2020_ST55.zip exists\n",
      "TAB2010_TAB2020_ST56.zip exists\n",
      "TAB2010_TAB2020_ST72.zip exists\n"
     ]
    }
   ],
   "source": [
    "# Find the 'ul' with a specific id\n",
    "div = soup.find(\"div\", {\"id\": \"data-uscb-state-list-selector\"})\n",
    "\n",
    "# # Extract all 'a' tags within the 'ul'\n",
    "links = div.find_all(\"a\") if div else []\n",
    "\n",
    "# Extract the href attribute from each link\n",
    "hrefs = [link.get(\"href\") for link in links]\n",
    "\n",
    "\n",
    "def download_file(href):\n",
    "    download_loc = os.path.join(data_path, \"Blocks20To10\")\n",
    "\n",
    "    # check if location exists\n",
    "    if not os.path.exists(download_loc):\n",
    "        # if the directory does not exist, create it\n",
    "        os.makedirs(download_loc)\n",
    "\n",
    "    # configure selenium and download path\n",
    "    options = Options()\n",
    "    options.set_preference(\"browser.download.folderList\", 2)\n",
    "    options.set_preference(\"browser.download.manager.showWhenStarting\", False)\n",
    "    options.set_preference(\"browser.download.dir\", download_loc)\n",
    "\n",
    "    download_filename = href.split(\"/\")[-1]\n",
    "    download_path = os.path.join(download_loc, download_filename)\n",
    "    temp_download_path = download_path + \".part\"\n",
    "\n",
    "    # Check if the download file already exists\n",
    "    if os.path.isfile(download_path):\n",
    "        print(f\"{download_filename} exists\")\n",
    "        return\n",
    "\n",
    "    # Start the Selenium driver\n",
    "    driver = webdriver.Firefox(options=options)\n",
    "    driver.set_page_load_timeout(5)\n",
    "\n",
    "    try:\n",
    "        driver.get(href)\n",
    "    except TimeoutException:\n",
    "        print(\"Page load timed out but continuing execution.\")\n",
    "\n",
    "    # Wait for the download to complete, checking every second\n",
    "    while True:\n",
    "        files = os.listdir(download_loc)\n",
    "        # Check if the .part file for the current download exists\n",
    "        if any(\n",
    "            file.startswith(download_filename.split(\".\")[0]) and file.endswith(\".part\")\n",
    "            for file in files\n",
    "        ):\n",
    "            print(f\"Download in progress for {download_filename}...\")\n",
    "            time.sleep(1)\n",
    "        else:\n",
    "            # If there's no .part file, the download is complete\n",
    "            print(f\"Download completed for {download_filename}.\")\n",
    "            break\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "\n",
    "# Use ThreadPoolExecutor to download files simultaneously\n",
    "with ThreadPoolExecutor(\n",
    "    max_workers=8\n",
    ") as executor:  # Adjust the number of workers as needed\n",
    "    executor.map(download_file, hrefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e91c9f-77dc-4f55-92ca-49848ca165f9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Map the FCC 2010 Blockcodes to 2020 Block Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5cf929d-b4f4-4f63-b71b-3abd38b4b0ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read the files\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "133618ab-c572-4f57-8c95-888ff237a686",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fips to state abbr relationship file\n",
    "\n",
    "fips_st_df = pd.read_csv(\n",
    "    os.path.join(data_path, \"fips_states\", \"us-state-ansi-fips.csv\")\n",
    ")\n",
    "fips_st_df.columns = [col.strip() for col in fips_st_df.columns]\n",
    "fips_st_df.st = fips_st_df.st.apply(lambda x: str(x).zfill(2))\n",
    "fips_st_df.stusps = fips_st_df.stusps.str.strip()\n",
    "\n",
    "# create a mapping dictionary\n",
    "fips_st_dict = dict(\n",
    "    zip(\n",
    "        fips_st_df.stusps,\n",
    "        fips_st_df.st,\n",
    "    )\n",
    ")\n",
    "# fips_st_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dba45d25-6fa9-4b61-b95c-7a6270afe091",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def zipfile_reader(zip_path):\n",
    "    with ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        # extract the CSV file name from the zip file\n",
    "        contained_file = zip_ref.namelist()[0]\n",
    "\n",
    "        # determine the delimiter based on the file extension\n",
    "        _, ext = os.path.splitext(contained_file)\n",
    "        delimiter = \"|\" if ext.lower() == \".txt\" else \",\"\n",
    "\n",
    "        # open the file within the zip\n",
    "        with zip_ref.open(contained_file) as csvfile:\n",
    "            # read the file into pandas with the appropriate delimiter\n",
    "            df = pd.read_csv(csvfile, delimiter=delimiter)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def process_18_17(fcc_path, rlnshp_path, is_partially_processed=False):\n",
    "    # year in 2019, 2020, 2021\n",
    "    if is_partially_processed:\n",
    "        df_ = pd.read_csv(fcc_path)\n",
    "\n",
    "    else:\n",
    "        df = zipfile_reader(fcc_path)\n",
    "\n",
    "        # latest tech\n",
    "        df[\"IsLatestTech\"] = df[\"TechCode\"].isin(latest_tech_codes)\n",
    "\n",
    "        # consumer dataframes\n",
    "        consumer_df = df[df[\"Consumer\"] == 1]\n",
    "\n",
    "        # group by 'BlockCode' and calculate the median 'MaxAdDown' and 'MaxAdUp'\n",
    "        block_speeds = (\n",
    "            consumer_df.groupby(\"BlockCode\")[[\"MaxAdDown\", \"MaxAdUp\"]]\n",
    "            .median()\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "        # calculate tech ratio\n",
    "        tech_ratio = (\n",
    "            consumer_df.groupby(\"BlockCode\")[\"IsLatestTech\"]\n",
    "            .mean()\n",
    "            .reset_index(name=\"LTRatio\")\n",
    "        )\n",
    "\n",
    "        df_ = block_speeds.merge(tech_ratio, on=\"BlockCode\")\n",
    "\n",
    "        df_.rename(\n",
    "            columns={\n",
    "                \"MaxAdDown\": \"MaxAdDown\" + year,\n",
    "                \"MaxAdUp\": \"MaxAdUp\" + year,\n",
    "                \"LTRatio\": \"LTRatio\" + year,\n",
    "            },\n",
    "            inplace=True,\n",
    "        )\n",
    "\n",
    "    # Get the relationship df of the 2010 and 2020 blocks\n",
    "    df_2 = zipfile_reader(rlnshp_path)\n",
    "\n",
    "    # concatenation using vectorized string operations\n",
    "    df_2[\"BLKCODE2010\"] = (\n",
    "        df_2.STATE_2010.astype(str).str.zfill(2)\n",
    "        + df_2.COUNTY_2010.astype(str).str.zfill(3)\n",
    "        + df_2.TRACT_2010.astype(str).str.zfill(6)\n",
    "        + df_2.BLK_2010.astype(str).str.zfill(4)\n",
    "    )\n",
    "\n",
    "    df_2[\"BLKCODE2020\"] = (\n",
    "        df_2.STATE_2020.astype(str).str.zfill(2)\n",
    "        + df_2.COUNTY_2020.astype(str).str.zfill(3)\n",
    "        + df_2.TRACT_2020.astype(str).str.zfill(6)\n",
    "        + df_2.BLK_2020.astype(str).str.zfill(4)\n",
    "    )\n",
    "\n",
    "    df_2[[\"BLKCODE2020\", \"BLKCODE2010\"]] = df_2[[\"BLKCODE2020\", \"BLKCODE2010\"]].astype(\n",
    "        int\n",
    "    )\n",
    "\n",
    "    # Create the relationship dictionary\n",
    "    relationship_dict = dict(zip(df_2.BLKCODE2010, df_2.BLKCODE2020))\n",
    "\n",
    "    # Filter the DataFrame to include only rows where BlockCode is a key in relationship_dict\n",
    "    df_filtered = df_[df_.BlockCode.isin(relationship_dict)]\n",
    "\n",
    "    # Map BlockCode to BLKCODE2020 using the relationship_dict\n",
    "    df_filtered[\"BLKCODE2020\"] = df_filtered.BlockCode.map(relationship_dict)\n",
    "\n",
    "    return df_filtered, relationship_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc96fc27-e64a-4202-9d6d-dd9cd54fbc99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use the multiprocessing and threading to speed file executions\n",
    "\n",
    "\n",
    "def process_files_for_year(year, batch_size=2):\n",
    "    files = os.listdir(os.path.join(data_path, year))\n",
    "\n",
    "    partially_proc_year = [\"2019\", \"2020\", \"2021\"]\n",
    "\n",
    "    def process_file(file):\n",
    "        if year in partially_proc_year:\n",
    "            is_partially_processed = True\n",
    "            state_abbr = file.split(\"_\")[0]\n",
    "            file_ending = file.endswith(\".csv\")\n",
    "        else:\n",
    "            is_partially_processed = False\n",
    "            state_abbr = file.split(\"-\")[0]\n",
    "            file_ending = file.endswith(\".zip\")\n",
    "\n",
    "        if state_abbr in list(fips_st_dict.keys()) and file_ending:\n",
    "            # Path to state fcc deployment data\n",
    "            fcc_path = os.path.join(data_path, year, file)\n",
    "            file_fips = fips_st_dict[state_abbr]\n",
    "\n",
    "            # Relationship file path\n",
    "            rlnshp_name = \"TAB2010_TAB2020_ST\" + file_fips + \".zip\"\n",
    "            rlnshp_path = os.path.join(data_path, \"Blocks20To10\", rlnshp_name)\n",
    "\n",
    "            df, relationship_dict = process_18_17(\n",
    "                fcc_path, rlnshp_path, is_partially_processed\n",
    "            )\n",
    "\n",
    "            # path to save the file\n",
    "            save_path = os.path.join(data_path, year + \"_processed\")\n",
    "\n",
    "            if not os.path.exists(save_path):\n",
    "                os.makedirs(save_path)\n",
    "\n",
    "            output_path = os.path.join(save_path, state_abbr + \"_\" + year + \".csv\")\n",
    "\n",
    "            if not os.path.exists(output_path):\n",
    "                # save the file for future use\n",
    "                df.to_csv(output_path)\n",
    "\n",
    "    # process files in batches\n",
    "    for i in range(0, len(files), batch_size):\n",
    "        batch_files = files[i : i + batch_size]\n",
    "        with ThreadPoolExecutor(max_workers=batch_size) as executor:\n",
    "            future_to_file = {\n",
    "                executor.submit(process_file, file): file for file in batch_files\n",
    "            }\n",
    "            for future in as_completed(future_to_file):\n",
    "                file = future_to_file[future]\n",
    "                try:\n",
    "                    data = future.result()\n",
    "                except Exception as exc:\n",
    "                    print(f\"{file} generated an exception: {exc}\")\n",
    "                else:\n",
    "                    print(type(data))\n",
    "                    # print(f'{file} is {len(data)} bytes')\n",
    "                # Explicitly handle garbage collection\n",
    "                gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf02d174-08ed-4461-ae26-827d81fadae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# years = [\"2017\", \"2018\", \"2019\", \"2020\", \"2021\"]\n",
    "# processes = []\n",
    "# max_processes = len(years)  # adjust as needed\n",
    "\n",
    "# for year in years:\n",
    "#     if len(processes) >= max_processes:\n",
    "#         for proc in processes:\n",
    "#             proc.join()\n",
    "#             processes.remove(proc)\n",
    "#             gc.collect()  # collect garbage to free memory\n",
    "#     process = Process(target=process_files_for_year, args=(year,))\n",
    "#     processes.append(process)\n",
    "#     process.start()\n",
    "\n",
    "# # wait for all processes to finish\n",
    "# for process in processes:\n",
    "#     process.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6bd643-7783-4b13-9340-f4d37ee8c81f",
   "metadata": {},
   "source": [
    "### Clean other demographic factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "109fcd21-7dc8-4885-b533-50581b67c6f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# clean income data, esp with uncertainties such as 25000-, 250000+ and - (nans)\n",
    "\n",
    "\n",
    "def transform_income(value):\n",
    "    if type(value) == str:\n",
    "        # Remove commas\n",
    "        value = value.replace(\",\", \"\")\n",
    "\n",
    "        # Handle different cases\n",
    "        if \"-\" in value:\n",
    "            if value.endswith(\"-\") and len(value) != 1:\n",
    "                # One-sided range, use the provided value\n",
    "                return float(value.replace(\"-\", \"\"))\n",
    "            elif value.startswith(\"-\"):\n",
    "                # Missing or uncertain data\n",
    "                return None  # Or use 0, or a specific strategy for missing data\n",
    "            else:\n",
    "                # Range, take the average\n",
    "                low, high = value.split(\"-\")\n",
    "                return (float(low) + float(high)) / 2\n",
    "        elif \"+\" in value:\n",
    "            # Open-ended value, use the provided number\n",
    "            return float(value.replace(\"+\", \"\"))\n",
    "        elif value == \"-\":\n",
    "            # Missing or uncertain data\n",
    "            return None  # Or use mean/median of the column\n",
    "        else:\n",
    "            # Regular value\n",
    "            return float(value)\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0fd04c9e-b8cc-4c65-9598-1c3041db37f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "income_data_path = os.path.join(\n",
    "    data_path, \"median_income\", \"ACSDT5Y2021.B19049-Data.csv\"\n",
    ")\n",
    "edu_data_path = os.path.join(data_path, \"education\", \"ACSDT5Y2021.B15003-Data.csv\")\n",
    "urban_rural_data_path = os.path.join(\n",
    "    data_path, \"urban_rural\", \"DECENNIALDHC2020.P2-Data.csv\"\n",
    ")\n",
    "internet_avl_data_path = os.path.join(\n",
    "    data_path, \"internet_availability\", \"ACSDT5Y2021.B28002-Data.csv\"\n",
    ")\n",
    "\n",
    "# read the csv files\n",
    "# income data\n",
    "income_df = pd.read_csv(income_data_path, usecols=[\"GEO_ID\", \"NAME\", \"B19049_001E\"])\n",
    "income_df.rename(columns={\"B19049_001E\": \"median_income\"}, inplace=True)\n",
    "income_df.drop(0, inplace=True)\n",
    "income_df[\"median_income\"] = income_df[\"median_income\"].apply(transform_income)\n",
    "income_df[\"median_income\"].fillna(income_df[\"median_income\"].mean(), inplace=True)\n",
    "\n",
    "# education data\n",
    "edu_df = pd.read_csv(\n",
    "    edu_data_path, usecols=[\"GEO_ID\", \"NAME\", \"B15003_001E\", \"B15003_022E\"]\n",
    ")\n",
    "edu_df.drop(0, inplace=True)\n",
    "edu_df[\"with_degree\"] = edu_df[\"B15003_022E\"].astype(int) / edu_df[\n",
    "    \"B15003_001E\"\n",
    "].astype(int)\n",
    "\n",
    "# ratio of rural to urban\n",
    "urban_df = pd.read_csv(\n",
    "    urban_rural_data_path, usecols=[\"GEO_ID\", \"NAME\", \"P2_001N\", \"P2_002N\"]\n",
    ")\n",
    "urban_df.drop(0, inplace=True)\n",
    "urban_df[\"urban_pop\"] = urban_df[\"P2_002N\"].astype(int) / urban_df[\"P2_001N\"].astype(\n",
    "    int\n",
    ")\n",
    "\n",
    "# with broadband of any type\n",
    "int_avl_df = pd.read_csv(\n",
    "    internet_avl_data_path, usecols=[\"GEO_ID\", \"NAME\", \"B28002_001E\", \"B28002_004E\"]\n",
    ")\n",
    "int_avl_df.drop(0, inplace=True)\n",
    "int_avl_df[\"with_broadband\"] = int_avl_df[\"B28002_004E\"].astype(int) / int_avl_df[\n",
    "    \"B28002_001E\"\n",
    "].astype(int)\n",
    "\n",
    "# merge the datasets and extract the columns of interest\n",
    "\n",
    "data = (\n",
    "    income_df[[\"GEO_ID\", \"median_income\"]]\n",
    "    .merge(edu_df[[\"GEO_ID\", \"with_degree\"]], on=\"GEO_ID\")\n",
    "    .merge(urban_df[[\"GEO_ID\", \"urban_pop\"]], on=\"GEO_ID\")\n",
    "    .merge(int_avl_df[[\"GEO_ID\", \"with_broadband\"]], on=\"GEO_ID\")\n",
    ")\n",
    "\n",
    "output_path = os.path.join(data_path, \"in_urb_ed_brb.csv\")\n",
    "\n",
    "# get census track\n",
    "data[\"census_tract\"] = data.GEO_ID.str[9:20]\n",
    "data.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d33c9c0f-df3e-4d8b-b4b9-9853d4d87f7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GEO_ID</th>\n",
       "      <th>NAME</th>\n",
       "      <th>median_income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1500000US010010201001</td>\n",
       "      <td>Block Group 1, Census Tract 201, Autauga Count...</td>\n",
       "      <td>41607.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1500000US010010201002</td>\n",
       "      <td>Block Group 2, Census Tract 201, Autauga Count...</td>\n",
       "      <td>66313.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1500000US010010202001</td>\n",
       "      <td>Block Group 1, Census Tract 202, Autauga Count...</td>\n",
       "      <td>42288.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1500000US010010202002</td>\n",
       "      <td>Block Group 2, Census Tract 202, Autauga Count...</td>\n",
       "      <td>52609.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1500000US010010203001</td>\n",
       "      <td>Block Group 1, Census Tract 203, Autauga Count...</td>\n",
       "      <td>75074.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242331</th>\n",
       "      <td>1500000US721537506011</td>\n",
       "      <td>Block Group 1, Census Tract 7506.01, Yauco Mun...</td>\n",
       "      <td>23050.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242332</th>\n",
       "      <td>1500000US721537506012</td>\n",
       "      <td>Block Group 2, Census Tract 7506.01, Yauco Mun...</td>\n",
       "      <td>32538.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242333</th>\n",
       "      <td>1500000US721537506013</td>\n",
       "      <td>Block Group 3, Census Tract 7506.01, Yauco Mun...</td>\n",
       "      <td>24864.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242334</th>\n",
       "      <td>1500000US721537506021</td>\n",
       "      <td>Block Group 1, Census Tract 7506.02, Yauco Mun...</td>\n",
       "      <td>11202.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242335</th>\n",
       "      <td>1500000US721537506022</td>\n",
       "      <td>Block Group 2, Census Tract 7506.02, Yauco Mun...</td>\n",
       "      <td>19457.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>242335 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       GEO_ID  \\\n",
       "1       1500000US010010201001   \n",
       "2       1500000US010010201002   \n",
       "3       1500000US010010202001   \n",
       "4       1500000US010010202002   \n",
       "5       1500000US010010203001   \n",
       "...                       ...   \n",
       "242331  1500000US721537506011   \n",
       "242332  1500000US721537506012   \n",
       "242333  1500000US721537506013   \n",
       "242334  1500000US721537506021   \n",
       "242335  1500000US721537506022   \n",
       "\n",
       "                                                     NAME  median_income  \n",
       "1       Block Group 1, Census Tract 201, Autauga Count...        41607.0  \n",
       "2       Block Group 2, Census Tract 201, Autauga Count...        66313.0  \n",
       "3       Block Group 1, Census Tract 202, Autauga Count...        42288.0  \n",
       "4       Block Group 2, Census Tract 202, Autauga Count...        52609.0  \n",
       "5       Block Group 1, Census Tract 203, Autauga Count...        75074.0  \n",
       "...                                                   ...            ...  \n",
       "242331  Block Group 1, Census Tract 7506.01, Yauco Mun...        23050.0  \n",
       "242332  Block Group 2, Census Tract 7506.01, Yauco Mun...        32538.0  \n",
       "242333  Block Group 3, Census Tract 7506.01, Yauco Mun...        24864.0  \n",
       "242334  Block Group 1, Census Tract 7506.02, Yauco Mun...        11202.0  \n",
       "242335  Block Group 2, Census Tract 7506.02, Yauco Mun...        19457.0  \n",
       "\n",
       "[242335 rows x 3 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "income_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f773d5-22dc-4184-aa5a-7dcee2cc870e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:us-broadband]",
   "language": "python",
   "name": "conda-env-us-broadband-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
