{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93a60c32-3041-4583-8428-e4dc480de4b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os\n",
    "from io import StringIO\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from multiprocessing import Process, current_process, Pool\n",
    "import gc\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "194a8fb7-ec1d-4537-9189-4796e38c3bf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# configure data path\n",
    "\n",
    "current_path = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "data_path = os.path.join(current_path, \"..\", \"data\")\n",
    "\n",
    "data_path = os.path.normpath(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66370d40-66f2-4bbb-b342-fea6e1db9a6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scrape_pages(loc, link):\n",
    "    \n",
    "    \n",
    "    download_loc = os.path.join(data_path, loc)\n",
    "\n",
    "    # check if location exists\n",
    "    if not os.path.exists(download_loc):\n",
    "        # if the directory does not exist, create it\n",
    "        os.makedirs(download_loc)\n",
    "\n",
    "    # configure selenium and download path\n",
    "    options = Options()\n",
    "    options.set_preference(\"browser.download.folderList\", 2)  \n",
    "    options.set_preference(\"browser.download.manager.showWhenStarting\", False)\n",
    "    options.set_preference(\"browser.download.dir\", download_loc)\n",
    "\n",
    "    # get the state page with requests\n",
    "    html_content = requests.get(link)\n",
    "\n",
    "    # beautify the page\n",
    "    soup = BeautifulSoup(html_content.text, 'html.parser')\n",
    "\n",
    "    # get tables in the hrml page with links to state broadband data\n",
    "    tables = soup.find_all('table')[1]\n",
    "    links = tables.find_all('a')\n",
    "\n",
    "    # loop through the links and download the files using selenium\n",
    "    for link in links:\n",
    "        href = link.get('href')\n",
    "\n",
    "        download_filename = href.split('/')[-1]\n",
    "        download_path = os.path.join(download_loc, download_filename)\n",
    "        temp_download_path = download_path + \".part\"\n",
    "\n",
    "        # check if download files exist\n",
    "        if os.path.isfile(download_path):\n",
    "            print(f'{download_filename} exist')\n",
    "        else:\n",
    "            # selenium driver\n",
    "            driver = webdriver.Firefox(options=options)\n",
    "            driver.set_page_load_timeout(5)\n",
    "            try:\n",
    "                driver.get(href)\n",
    "            except TimeoutException:\n",
    "                print(\"Page load timed out but continuing execution.\")\n",
    "\n",
    "            # wait for the download to complete, checking every second\n",
    "            while True:\n",
    "                \n",
    "                files = os.listdir(download_loc)\n",
    "                # Check if the .part file for the current download exists\n",
    "                if any(file.startswith(download_filename.split(\".\")[0]) and file.endswith('.part') for file in files):\n",
    "                    print(f\"Download in progress for {download_filename}...\")\n",
    "                    time.sleep(1)\n",
    "                else:\n",
    "                    # If there's no .part file, the download is complete\n",
    "                    print(f\"Download completed for {download_filename}.\")\n",
    "                    break\n",
    "\n",
    "            driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb5aa835-a0ea-47d6-8264-3312a55ac219",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AK-Fixed-Dec2017.zip exist\n",
      "AK-Fixed-Dec2018.zip existAL-Fixed-Dec2017.zip exist\n",
      "\n",
      "AR-Fixed-Dec2017.zip existAL-Fixed-Dec2018.zip exist\n",
      "AS-Fixed-Dec2017.zip exist\n",
      "\n",
      "AZ-Fixed-Dec2017.zip exist\n",
      "AR-Fixed-Dec2018.zip existCA-Fixed-Dec2017.zip exist\n",
      "\n",
      "CO-Fixed-Dec2017.zip existAS-Fixed-Dec2018.zip exist\n",
      "\n",
      "CT-Fixed-Dec2017.zip existAZ-Fixed-Dec2018.zip exist\n",
      "\n",
      "CA-Fixed-Dec2018.zip existDC-Fixed-Dec2017.zip exist\n",
      "\n",
      "DE-Fixed-Dec2017.zip existCO-Fixed-Dec2018.zip exist\n",
      "\n",
      "CT-Fixed-Dec2018.zip existFL-Fixed-Dec2017.zip exist\n",
      "\n",
      "GA-Fixed-Dec2017.zip existDC-Fixed-Dec2018.zip exist\n",
      "\n",
      "GU-Fixed-Dec2017.zip exist\n",
      "DE-Fixed-Dec2018.zip existHI-Fixed-Dec2017.zip exist\n",
      "\n",
      "IA-Fixed-Dec2017.zip existFL-Fixed-Dec2018.zip exist\n",
      "\n",
      "ID-Fixed-Dec2017.zip existGA-Fixed-Dec2018.zip exist\n",
      "\n",
      "IL-Fixed-Dec2017.zip exist\n",
      "GU-Fixed-Dec2018.zip exist\n",
      "IN-Fixed-Dec2017.zip exist\n",
      "HI-Fixed-Dec2018.zip existKS-Fixed-Dec2017.zip exist\n",
      "\n",
      "KY-Fixed-Dec2017.zip existIA-Fixed-Dec2018.zip exist\n",
      "\n",
      "ID-Fixed-Dec2018.zip existLA-Fixed-Dec2017.zip exist\n",
      "\n",
      "IL-Fixed-Dec2018.zip existMA-Fixed-Dec2017.zip exist\n",
      "\n",
      "MD-Fixed-Dec2017.zip existIN-Fixed-Dec2018.zip exist\n",
      "\n",
      "KS-Fixed-Dec2018.zip existME-Fixed-Dec2017.zip exist\n",
      "\n",
      "KY-Fixed-Dec2018.zip exist\n",
      "MI-Fixed-Dec2017.zip existLA-Fixed-Dec2018.zip exist\n",
      "\n",
      "MN-Fixed-Dec2017.zip existMA-Fixed-Dec2018.zip exist\n",
      "\n",
      "MO-Fixed-Dec2017.zip existMD-Fixed-Dec2018.zip exist\n",
      "\n",
      "ME-Fixed-Dec2018.zip existMP-Fixed-Dec2017.zip exist\n",
      "\n",
      "MI-Fixed-Dec2018.zip existMS-Fixed-Dec2017.zip exist\n",
      "\n",
      "MN-Fixed-Dec2018.zip existMT-Fixed-Dec2017.zip exist\n",
      "\n",
      "MO-Fixed-Dec2018.zip existNC-Fixed-Dec2017.zip exist\n",
      "\n",
      "MP-Fixed-Dec2018.zip existND-Fixed-Dec2017.zip exist\n",
      "\n",
      "MS-Fixed-Dec2018.zip existNE-Fixed-Dec2017.zip exist\n",
      "\n",
      "MT-Fixed-Dec2018.zip existNH-Fixed-Dec2017.zip exist\n",
      "\n",
      "NJ-Fixed-Dec2017.zip existNC-Fixed-Dec2018.zip exist\n",
      "\n",
      "NM-Fixed-Dec2017.zip existND-Fixed-Dec2018.zip exist\n",
      "\n",
      "NE-Fixed-Dec2018.zip existNV-Fixed-Dec2017.zip exist\n",
      "\n",
      "NH-Fixed-Dec2018.zip exist\n",
      "NY-Fixed-Dec2017.zip existNJ-Fixed-Dec2018.zip exist\n",
      "\n",
      "NM-Fixed-Dec2018.zip existOH-Fixed-Dec2017.zip exist\n",
      "\n",
      "NV-Fixed-Dec2018.zip existOK-Fixed-Dec2017.zip exist\n",
      "\n",
      "NY-Fixed-Dec2018.zip existOR-Fixed-Dec2017.zip exist\n",
      "\n",
      "OH-Fixed-Dec2018.zip existPA-Fixed-Dec2017.zip exist\n",
      "\n",
      "PR-Fixed-Dec2017.zip existOK-Fixed-Dec2018.zip exist\n",
      "\n",
      "RI-Fixed-Dec2017.zip existOR-Fixed-Dec2018.zip exist\n",
      "\n",
      "SC-Fixed-Dec2017.zip existPA-Fixed-Dec2018.zip exist\n",
      "\n",
      "SD-Fixed-Dec2017.zip existPR-Fixed-Dec2018.zip exist\n",
      "\n",
      "TN-Fixed-Dec2017.zip existRI-Fixed-Dec2018.zip exist\n",
      "\n",
      "TX-Fixed-Dec2017.zip exist\n",
      "SC-Fixed-Dec2018.zip exist\n",
      "UT-Fixed-Dec2017.zip existSD-Fixed-Dec2018.zip exist\n",
      "\n",
      "VA-Fixed-Dec2017.zip existTN-Fixed-Dec2018.zip exist\n",
      "\n",
      "VI-Fixed-Dec2017.zip existTX-Fixed-Dec2018.zip exist\n",
      "\n",
      "UT-Fixed-Dec2018.zip exist\n",
      "VT-Fixed-Dec2017.zip existVA-Fixed-Dec2018.zip exist\n",
      "\n",
      "WA-Fixed-Dec2017.zip existVI-Fixed-Dec2018.zip exist\n",
      "\n",
      "WI-Fixed-Dec2017.zip exist\n",
      "WV-Fixed-Dec2017.zip exist\n",
      "WY-Fixed-Dec2017.zip exist\n",
      "Page load timed out but continuing execution.\n",
      "Download completed for VT-Fixed-Dec2018.zip.\n",
      "WA-Fixed-Dec2018.zip exist\n",
      "WI-Fixed-Dec2018.zip exist\n",
      "WV-Fixed-Dec2018.zip exist\n",
      "WY-Fixed-Dec2018.zip exist\n"
     ]
    }
   ],
   "source": [
    "# using multiprocessor\n",
    "def scrape_wrapper(args):\n",
    "    return scrape_pages(*args)\n",
    "\n",
    "# List of arguments for each dataset\n",
    "datasets = [\n",
    "    (\"2017\", \"https://www.fcc.gov/form-477-broadband-deployment-data-december-2017-version-3\"),\n",
    "    (\"2018\", \"https://www.fcc.gov/form-477-broadband-deployment-data-december-2018-version-3\"),\n",
    "    # (\"2020\", \"https://www.fcc.gov/form-477-broadband-deployment-data-december-2020\"),\n",
    "]\n",
    "\n",
    "# Using Pool to create a pool of worker processes\n",
    "with Pool(processes=len(datasets)) as pool:\n",
    "    # map the datasets to the scrape_wrapper function\n",
    "    pool.map(scrape_wrapper, datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc7da47e-57ab-4cdf-8e57-75e2a675a719",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# break the 2020, 2021, and 2019 large fcc deployment datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "192c7e92-0533-4a93-be63-a23121d57dc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chunk_size = 200000\n",
    "\n",
    "# latest technologies\n",
    "latest_tech_codes = [43, 50]\n",
    "\n",
    "def break_csv(year, file_name):\n",
    "    \n",
    "    csv_path = os.path.join(data_path, file_name)\n",
    "    file_path = os.path.join(data_path, year)\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        os.makedirs(file_path)\n",
    "    \n",
    "    # use an iterator to read in chunks\n",
    "    for chunk in tqdm(pd.read_csv(os.path.join(data_path, \"fbd_us_without_satellite_dec2019_v1.csv\"), chunksize=chunk_size, encoding='utf8', encoding_errors='ignore')):\n",
    "\n",
    "        # Process each chunk\n",
    "        for state_abbr, group_df in chunk.groupby('StateAbbr'):\n",
    "            # Define the filename for each state's CSV\n",
    "            filename = f\"{state_abbr}_{year}.csv\"\n",
    "            \n",
    "            # latest tech\n",
    "            group_df['IsLatestTech'] = group_df['TechCode'].isin(latest_tech_codes)\n",
    "            \n",
    "            # consumer dataframes\n",
    "            consumer_df = group_df[group_df['Consumer'] == 1]\n",
    "            \n",
    "            if consumer_df.shape[0] == 0:\n",
    "                continue\n",
    "            # Group by 'BlockCode' and calculate the median 'MaxAdDown' and 'MaxAdUp'\n",
    "            block_speeds = consumer_df.groupby('BlockCode')[['MaxAdDown', 'MaxAdUp']].median().reset_index()\n",
    "\n",
    "            tech_ratio = consumer_df.groupby('BlockCode').apply(\n",
    "            lambda x: x['IsLatestTech'].sum() / len(x)).reset_index(name='LTRatio')\n",
    "\n",
    "            df_ = block_speeds.merge(tech_ratio, on=\"BlockCode\")\n",
    "            \n",
    "            df_.rename(columns={\"MaxAdDown\": \"MaxAdDown\"+year, \"MaxAdUp\": \"MaxAdUp\"+year, \"LTRatio\": \"LTRatio\"+year}, inplace=True)\n",
    "\n",
    "            # Append data to the CSV if it already exists, else create a new one\n",
    "            with open(os.path.join(file_path, filename), 'a') as f:\n",
    "                df_.to_csv(f, index=False, header=f.tell()==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e6fd90a-7a92-4c06-88c1-9b28cdc63da8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def multiprocessor_wrapper(args):\n",
    "    return break_csv(*args)\n",
    "    \n",
    "data = [(\"2019\", \"fbd_us_without_satellite_dec2019_v1.csv\"),\n",
    "       (\"2020\", \"fbd_us_without_satellite_dec2020_v1.csv\"),\n",
    "       (\"2021\", \"fbd_us_without_satellite_dec2021_v1.csv\")]\n",
    "\n",
    "# with Pool(processes=len(data)) as pool:\n",
    "#     # map the datasets to the scrape_wrapper function\n",
    "#     pool.map(multiprocessor_wrapper, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a208401-5e31-4e93-be00-57a78270c576",
   "metadata": {},
   "source": [
    "### Download 2020 and 2010 census block relationship files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1c260b0-f1e3-4e3f-9263-45ed846ddba6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# download the 2020 2010 block codes concordances\n",
    "\n",
    "# get the state page with requests\n",
    "html_content = requests.get(\"https://www.census.gov/geographies/reference-files/time-series/geo/relationship-files.2020.html#blkgrp\")\n",
    "\n",
    "# beautify the page\n",
    "soup = BeautifulSoup(html_content.text, 'html.parser')\n",
    "\n",
    "# # get tables in the hrml page with links to state broadband data\n",
    "# tables = soup.find_all('table')[1]\n",
    "# links = tables.find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c9c453d-a25e-49e0-bbb6-ee37f62bdb91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TAB2010_TAB2020_ST02.zip existsTAB2010_TAB2020_ST04.zip exists\n",
      "TAB2010_TAB2020_ST01.zip exists\n",
      "TAB2010_TAB2020_ST05.zip exists\n",
      "TAB2010_TAB2020_ST06.zip exists\n",
      "\n",
      "TAB2010_TAB2020_ST08.zip exists\n",
      "TAB2010_TAB2020_ST10.zip exists\n",
      "TAB2010_TAB2020_ST09.zip exists\n",
      "TAB2010_TAB2020_ST12.zip exists\n",
      "TAB2010_TAB2020_ST11.zip exists\n",
      "TAB2010_TAB2020_ST13.zip exists\n",
      "TAB2010_TAB2020_ST16.zip exists\n",
      "TAB2010_TAB2020_ST17.zip exists\n",
      "TAB2010_TAB2020_ST18.zip exists\n",
      "TAB2010_TAB2020_ST19.zip exists\n",
      "TAB2010_TAB2020_ST20.zip exists\n",
      "TAB2010_TAB2020_ST21.zip exists\n",
      "TAB2010_TAB2020_ST26.zip exists\n",
      "TAB2010_TAB2020_ST23.zip exists\n",
      "TAB2010_TAB2020_ST15.zip exists\n",
      "TAB2010_TAB2020_ST24.zip exists\n",
      "TAB2010_TAB2020_ST22.zip exists\n",
      "TAB2010_TAB2020_ST27.zip exists\n",
      "TAB2010_TAB2020_ST28.zip exists\n",
      "TAB2010_TAB2020_ST25.zip exists\n",
      "TAB2010_TAB2020_ST32.zip exists\n",
      "TAB2010_TAB2020_ST30.zip exists\n",
      "TAB2010_TAB2020_ST33.zip exists\n",
      "TAB2010_TAB2020_ST31.zip exists\n",
      "TAB2010_TAB2020_ST34.zip exists\n",
      "TAB2010_TAB2020_ST36.zip exists\n",
      "TAB2010_TAB2020_ST29.zip exists\n",
      "TAB2010_TAB2020_ST38.zip exists\n",
      "TAB2010_TAB2020_ST35.zip exists\n",
      "TAB2010_TAB2020_ST40.zip exists\n",
      "TAB2010_TAB2020_ST39.zip exists\n",
      "TAB2010_TAB2020_ST37.zip exists\n",
      "TAB2010_TAB2020_ST41.zip exists\n",
      "TAB2010_TAB2020_ST42.zip exists\n",
      "TAB2010_TAB2020_ST44.zip exists\n",
      "TAB2010_TAB2020_ST45.zip exists\n",
      "TAB2010_TAB2020_ST46.zip exists\n",
      "TAB2010_TAB2020_ST49.zip exists\n",
      "TAB2010_TAB2020_ST48.zip exists\n",
      "TAB2010_TAB2020_ST50.zip exists\n",
      "TAB2010_TAB2020_ST53.zip exists\n",
      "TAB2010_TAB2020_ST55.zip exists\n",
      "TAB2010_TAB2020_ST51.zip exists\n",
      "TAB2010_TAB2020_ST47.zip exists\n",
      "TAB2010_TAB2020_ST54.zip exists\n",
      "TAB2010_TAB2020_ST72.zip exists\n",
      "TAB2010_TAB2020_ST56.zip exists\n"
     ]
    }
   ],
   "source": [
    "# Find the 'ul' with a specific id\n",
    "div = soup.find('div', {'id': 'data-uscb-state-list-selector'})\n",
    "\n",
    "# # Extract all 'a' tags within the 'ul'\n",
    "links = div.find_all('a') if div else []\n",
    "\n",
    "# Extract the href attribute from each link\n",
    "hrefs = [link.get('href') for link in links]\n",
    "\n",
    "\n",
    "def download_file(href):\n",
    "    \n",
    "    download_loc = os.path.join(data_path, \"Blocks20To10\")\n",
    "\n",
    "    # check if location exists\n",
    "    if not os.path.exists(download_loc):\n",
    "        # if the directory does not exist, create it\n",
    "        os.makedirs(download_loc)\n",
    "\n",
    "    # configure selenium and download path\n",
    "    options = Options()\n",
    "    options.set_preference(\"browser.download.folderList\", 2)  \n",
    "    options.set_preference(\"browser.download.manager.showWhenStarting\", False)\n",
    "    options.set_preference(\"browser.download.dir\", download_loc)\n",
    "\n",
    "    download_filename = href.split('/')[-1]\n",
    "    download_path = os.path.join(download_loc, download_filename)\n",
    "    temp_download_path = download_path + \".part\"\n",
    "\n",
    "    # Check if the download file already exists\n",
    "    if os.path.isfile(download_path):\n",
    "        print(f'{download_filename} exists')\n",
    "        return\n",
    "    \n",
    "    # Start the Selenium driver\n",
    "    driver = webdriver.Firefox(options=options)\n",
    "    driver.set_page_load_timeout(5)\n",
    "\n",
    "    try:\n",
    "        driver.get(href)\n",
    "    except TimeoutException:\n",
    "        print(\"Page load timed out but continuing execution.\")\n",
    "\n",
    "    # Wait for the download to complete, checking every second\n",
    "    while True:\n",
    "        files = os.listdir(download_loc)\n",
    "        # Check if the .part file for the current download exists\n",
    "        if any(file.startswith(download_filename.split(\".\")[0]) and file.endswith('.part') for file in files):\n",
    "            print(f\"Download in progress for {download_filename}...\")\n",
    "            time.sleep(1)\n",
    "        else:\n",
    "            # If there's no .part file, the download is complete\n",
    "            print(f\"Download completed for {download_filename}.\")\n",
    "            break\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "# Use ThreadPoolExecutor to download files simultaneously\n",
    "with ThreadPoolExecutor(max_workers=8) as executor:  # Adjust the number of workers as needed\n",
    "    executor.map(download_file, hrefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e91c9f-77dc-4f55-92ca-49848ca165f9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Map the FCC 2010 Blockcodes to 2020 Block Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b5cf929d-b4f4-4f63-b71b-3abd38b4b0ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read the files\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "133618ab-c572-4f57-8c95-888ff237a686",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fips to state abbr relationship file\n",
    "\n",
    "fips_st_df = pd.read_csv(os.path.join(data_path, \"fips_states\", \"us-state-ansi-fips.csv\"))\n",
    "fips_st_df.columns = [col.strip() for col in fips_st_df.columns]\n",
    "fips_st_df.st = fips_st_df.st.apply(lambda x: str(x).zfill(2))\n",
    "fips_st_df.stusps = fips_st_df.stusps.str.strip()\n",
    "\n",
    "# create a mapping dictionary\n",
    "fips_st_dict = dict(zip(fips_st_df.stusps,fips_st_df.st, ))\n",
    "# fips_st_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dba45d25-6fa9-4b61-b95c-7a6270afe091",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def zipfile_reader(zip_path):\n",
    "        \n",
    "    with ZipFile(zip_path, 'r') as zip_ref:\n",
    "        # extract the CSV file name from the zip file\n",
    "        contained_file = zip_ref.namelist()[0]\n",
    "\n",
    "        # determine the delimiter based on the file extension\n",
    "        _, ext = os.path.splitext(contained_file)\n",
    "        delimiter = \"|\" if ext.lower() == '.txt' else ','\n",
    "\n",
    "        # open the file within the zip\n",
    "        with zip_ref.open(contained_file) as csvfile:\n",
    "\n",
    "            # read the file into pandas with the appropriate delimiter\n",
    "            df = pd.read_csv(csvfile, delimiter=delimiter)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "\n",
    "def process_18_17(fcc_path, rlnshp_path, is_partially_processed=False):\n",
    "    \n",
    "    # year in 2019, 2020, 2021\n",
    "    if is_partially_processed:\n",
    "        \n",
    "        df_ = pd.read_csv(fcc_path)\n",
    "        \n",
    "    else:\n",
    "    \n",
    "        df = zipfile_reader(fcc_path)\n",
    "\n",
    "        # latest tech\n",
    "        df['IsLatestTech'] = df['TechCode'].isin(latest_tech_codes)\n",
    "\n",
    "        # consumer dataframes\n",
    "        consumer_df = df[df['Consumer'] == 1]\n",
    "\n",
    "        # group by 'BlockCode' and calculate the median 'MaxAdDown' and 'MaxAdUp'\n",
    "        block_speeds = consumer_df.groupby('BlockCode')[['MaxAdDown', 'MaxAdUp']].median().reset_index()\n",
    "\n",
    "        # calculate tech ratio\n",
    "        tech_ratio = consumer_df.groupby('BlockCode')['IsLatestTech'].mean().reset_index(name='LTRatio')\n",
    "\n",
    "        df_ = block_speeds.merge(tech_ratio, on=\"BlockCode\")\n",
    "\n",
    "        df_.rename(columns={\"MaxAdDown\": \"MaxAdDown\" + year, \"MaxAdUp\": \"MaxAdUp\" + year, \"LTRatio\": \"LTRatio\" + year}, inplace=True)\n",
    "\n",
    "        \n",
    "    # Get the relationship df of the 2010 and 2020 blocks\n",
    "    df_2 = zipfile_reader(rlnshp_path)\n",
    "\n",
    "    # concatenation using vectorized string operations\n",
    "    df_2[\"BLKCODE2010\"] = df_2.STATE_2010.astype(str).str.zfill(2) + df_2.COUNTY_2010.astype(str).str.zfill(3) + \\\n",
    "                          df_2.TRACT_2010.astype(str).str.zfill(6) + df_2.BLK_2010.astype(str).str.zfill(4)\n",
    "\n",
    "    df_2[\"BLKCODE2020\"] = df_2.STATE_2020.astype(str).str.zfill(2) + df_2.COUNTY_2020.astype(str).str.zfill(3) + \\\n",
    "                          df_2.TRACT_2020.astype(str).str.zfill(6) + df_2.BLK_2020.astype(str).str.zfill(4)\n",
    "\n",
    "    df_2[[\"BLKCODE2020\", \"BLKCODE2010\"]] = df_2[[\"BLKCODE2020\", \"BLKCODE2010\"]].astype(int)\n",
    "\n",
    "    # Create the relationship dictionary\n",
    "    relationship_dict = dict(zip(df_2.BLKCODE2010, df_2.BLKCODE2020))\n",
    "\n",
    "    # Filter the DataFrame to include only rows where BlockCode is a key in relationship_dict\n",
    "    df_filtered = df_[df_.BlockCode.isin(relationship_dict)]\n",
    "\n",
    "    # Map BlockCode to BLKCODE2020 using the relationship_dict\n",
    "    df_filtered[\"BLKCODE2020\"] = df_filtered.BlockCode.map(relationship_dict)\n",
    "\n",
    "    \n",
    "    return df_filtered, relationship_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc96fc27-e64a-4202-9d6d-dd9cd54fbc99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use the multiprocessing and threading to speed file executions\n",
    "\n",
    "def process_files_for_year(year, batch_size=2):\n",
    "    files = os.listdir(os.path.join(data_path, year))\n",
    "    \n",
    "    partially_proc_year = [\"2019\", \"2020\", \"2021\"]\n",
    "        \n",
    "    def process_file(file):\n",
    "            \n",
    "        if year in partially_proc_year:\n",
    "            is_partially_processed = True\n",
    "            state_abbr = file.split(\"_\")[0]\n",
    "            file_ending = file.endswith(\".csv\")\n",
    "        else:\n",
    "            is_partially_processed = False\n",
    "            state_abbr = file.split(\"-\")[0]\n",
    "            file_ending = file.endswith(\".zip\")\n",
    "            \n",
    "        if state_abbr in list(fips_st_dict.keys()) and file_ending:\n",
    "            # Path to state fcc deployment data\n",
    "            fcc_path = os.path.join(data_path, year, file)\n",
    "            file_fips = fips_st_dict[state_abbr]\n",
    "            \n",
    "            # Relationship file path\n",
    "            rlnshp_name = \"TAB2010_TAB2020_ST\" + file_fips + \".zip\"\n",
    "            rlnshp_path = os.path.join(data_path, \"Blocks20To10\", rlnshp_name)\n",
    "            \n",
    "            df, relationship_dict = process_18_17(fcc_path, rlnshp_path, is_partially_processed)\n",
    "            \n",
    "            # path to save the file\n",
    "            save_path = os.path.join(data_path, year+\"_processed\")\n",
    "            \n",
    "            if not os.path.exists(save_path):\n",
    "                \n",
    "                os.makedirs(save_path)\n",
    "            \n",
    "            output_path = os.path.join(save_path, state_abbr + \"_\" + year + \".csv\")\n",
    "            \n",
    "            if not os.path.exists(output_path):\n",
    "            \n",
    "                # save the file for future use\n",
    "                df.to_csv(output_path)\n",
    "\n",
    "    # process files in batches\n",
    "    for i in range(0, len(files), batch_size):\n",
    "        batch_files = files[i:i+batch_size]\n",
    "        with ThreadPoolExecutor(max_workers=batch_size) as executor:\n",
    "            future_to_file = {executor.submit(process_file, file): file for file in batch_files}\n",
    "            for future in as_completed(future_to_file):\n",
    "                file = future_to_file[future]\n",
    "                try:\n",
    "                    data = future.result()\n",
    "                except Exception as exc:\n",
    "                    print(f'{file} generated an exception: {exc}')\n",
    "                else:\n",
    "                    print(type(data))\n",
    "                    # print(f'{file} is {len(data)} bytes')\n",
    "                # Explicitly handle garbage collection\n",
    "                gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf02d174-08ed-4461-ae26-827d81fadae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [\"2017\", \"2018\", \"2019\", \"2020\", \"2021\"]\n",
    "processes = []\n",
    "max_processes = len(years)  # adjust as needed\n",
    "\n",
    "for year in years:\n",
    "    if len(processes) >= max_processes:\n",
    "        for proc in processes:\n",
    "            proc.join() \n",
    "            processes.remove(proc)\n",
    "            gc.collect()  # collect garbage to free memory\n",
    "    process = Process(target=process_files_for_year, args=(year,))\n",
    "    processes.append(process)\n",
    "    process.start()\n",
    "\n",
    "# wait for all processes to finish\n",
    "for process in processes:\n",
    "    process.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6bd643-7783-4b13-9340-f4d37ee8c81f",
   "metadata": {},
   "source": [
    "### Clean other demographic factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0fd04c9e-b8cc-4c65-9598-1c3041db37f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "income_data_path = os.path.join(data_path, \"median_income\", \"ACSDT5Y2021.B19049-Data.csv\")\n",
    "edu_data_path = os.path.join(data_path, \"education\", \"ACSDT5Y2021.B15003-Data.csv\")\n",
    "urban_rural_data_path = os.path.join(data_path, \"urban_rural\", \"DECENNIALDHC2020.P2-Data.csv\")\n",
    "internet_avl_data_path = os.path.join(data_path, \"internet_availability\", \"ACSDT5Y2021.B28002-Data.csv\")\n",
    "\n",
    "# read the csv files\n",
    "# income data\n",
    "income_df = pd.read_csv(income_data_path, usecols=['GEO_ID', 'NAME', 'B19049_001E'])\n",
    "income_df.rename(columns={\"B19049_001E\": \"median_income\"}, inplace=True)\n",
    "income_df.drop(0, inplace=True)\n",
    "\n",
    "# education data\n",
    "edu_df = pd.read_csv(edu_data_path, usecols=['GEO_ID', 'NAME', 'B15003_001E', \"B15003_022E\"])\n",
    "edu_df.drop(0, inplace=True)\n",
    "edu_df[\"with_degree\"] = edu_df[\"B15003_022E\"].astype(int) / edu_df[\"B15003_001E\"].astype(int)\n",
    "\n",
    "# ratio of rural to urban\n",
    "urban_df = pd.read_csv(urban_rural_data_path, usecols=['GEO_ID', 'NAME', 'P2_001N', 'P2_002N'])\n",
    "urban_df.drop(0, inplace=True)\n",
    "urban_df[\"urban_pop\"] = urban_df[\"P2_002N\"].astype(int) / urban_df[\"P2_001N\"].astype(int)\n",
    "\n",
    "# with broadband of any type\n",
    "int_avl_df = pd.read_csv(internet_avl_data_path, usecols=['GEO_ID', 'NAME', 'B28002_001E', \"B28002_004E\"])\n",
    "int_avl_df.drop(0, inplace=True)\n",
    "int_avl_df[\"with_broadband\"] = int_avl_df[\"B28002_004E\"].astype(int) / int_avl_df[\"B28002_001E\"].astype(int)\n",
    "\n",
    "# merge the datasets and extract the columns of interest\n",
    "\n",
    "data = income_df[[\"GEO_ID\", \"median_income\"]].merge(edu_df[[\"GEO_ID\", \"with_degree\"]], on=\"GEO_ID\").merge(urban_df[[\"GEO_ID\", \"urban_pop\"]], on=\"GEO_ID\").merge(int_avl_df[[\"GEO_ID\", \"with_broadband\"]], on=\"GEO_ID\")\n",
    "\n",
    "data_path = os.path.join(data_path, \"in_urb_ed_brb.csv\")\n",
    "data.to_csv(data_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c19418cb-f7ca-46b1-92bf-8820a59b52cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GEO_ID</th>\n",
       "      <th>median_income</th>\n",
       "      <th>with_degree</th>\n",
       "      <th>urban_pop</th>\n",
       "      <th>with_broadband</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1500000US010010201001</td>\n",
       "      <td>41607</td>\n",
       "      <td>0.157464</td>\n",
       "      <td>0.650435</td>\n",
       "      <td>0.793651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1500000US010010201002</td>\n",
       "      <td>66313</td>\n",
       "      <td>0.118207</td>\n",
       "      <td>0.935833</td>\n",
       "      <td>0.873950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1500000US010010202001</td>\n",
       "      <td>42288</td>\n",
       "      <td>0.124283</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.838235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1500000US010010202002</td>\n",
       "      <td>52609</td>\n",
       "      <td>0.125392</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.737805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1500000US010010203001</td>\n",
       "      <td>75074</td>\n",
       "      <td>0.107667</td>\n",
       "      <td>0.998738</td>\n",
       "      <td>0.910243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242330</th>\n",
       "      <td>1500000US721537506011</td>\n",
       "      <td>23050</td>\n",
       "      <td>0.391984</td>\n",
       "      <td>0.865230</td>\n",
       "      <td>0.654224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242331</th>\n",
       "      <td>1500000US721537506012</td>\n",
       "      <td>32538</td>\n",
       "      <td>0.295732</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.720971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242332</th>\n",
       "      <td>1500000US721537506013</td>\n",
       "      <td>24864</td>\n",
       "      <td>0.261168</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.612403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242333</th>\n",
       "      <td>1500000US721537506021</td>\n",
       "      <td>11202</td>\n",
       "      <td>0.103419</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.386992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242334</th>\n",
       "      <td>1500000US721537506022</td>\n",
       "      <td>19457</td>\n",
       "      <td>0.109272</td>\n",
       "      <td>0.548046</td>\n",
       "      <td>0.638060</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>242335 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       GEO_ID median_income  with_degree  urban_pop  \\\n",
       "0       1500000US010010201001         41607     0.157464   0.650435   \n",
       "1       1500000US010010201002         66313     0.118207   0.935833   \n",
       "2       1500000US010010202001         42288     0.124283   1.000000   \n",
       "3       1500000US010010202002         52609     0.125392   1.000000   \n",
       "4       1500000US010010203001         75074     0.107667   0.998738   \n",
       "...                       ...           ...          ...        ...   \n",
       "242330  1500000US721537506011         23050     0.391984   0.865230   \n",
       "242331  1500000US721537506012         32538     0.295732   1.000000   \n",
       "242332  1500000US721537506013         24864     0.261168   1.000000   \n",
       "242333  1500000US721537506021         11202     0.103419   1.000000   \n",
       "242334  1500000US721537506022         19457     0.109272   0.548046   \n",
       "\n",
       "        with_broadband  \n",
       "0             0.793651  \n",
       "1             0.873950  \n",
       "2             0.838235  \n",
       "3             0.737805  \n",
       "4             0.910243  \n",
       "...                ...  \n",
       "242330        0.654224  \n",
       "242331        0.720971  \n",
       "242332        0.612403  \n",
       "242333        0.386992  \n",
       "242334        0.638060  \n",
       "\n",
       "[242335 rows x 5 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e0f294d1-f2a5-41e3-9ca8-4be313560b66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GEO_ID</th>\n",
       "      <th>NAME</th>\n",
       "      <th>B28002_001E</th>\n",
       "      <th>B28002_004E</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Geography</td>\n",
       "      <td>Geographic Area Name</td>\n",
       "      <td>Estimate!!Total:</td>\n",
       "      <td>Estimate!!Total:!!With an Internet subscriptio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1500000US010010201001</td>\n",
       "      <td>Block Group 1, Census Tract 201, Autauga Count...</td>\n",
       "      <td>252</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1500000US010010201002</td>\n",
       "      <td>Block Group 2, Census Tract 201, Autauga Count...</td>\n",
       "      <td>357</td>\n",
       "      <td>312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1500000US010010202001</td>\n",
       "      <td>Block Group 1, Census Tract 202, Autauga Count...</td>\n",
       "      <td>272</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1500000US010010202002</td>\n",
       "      <td>Block Group 2, Census Tract 202, Autauga Count...</td>\n",
       "      <td>328</td>\n",
       "      <td>242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242331</th>\n",
       "      <td>1500000US721537506011</td>\n",
       "      <td>Block Group 1, Census Tract 7506.01, Yauco Mun...</td>\n",
       "      <td>509</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242332</th>\n",
       "      <td>1500000US721537506012</td>\n",
       "      <td>Block Group 2, Census Tract 7506.01, Yauco Mun...</td>\n",
       "      <td>577</td>\n",
       "      <td>416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242333</th>\n",
       "      <td>1500000US721537506013</td>\n",
       "      <td>Block Group 3, Census Tract 7506.01, Yauco Mun...</td>\n",
       "      <td>387</td>\n",
       "      <td>237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242334</th>\n",
       "      <td>1500000US721537506021</td>\n",
       "      <td>Block Group 1, Census Tract 7506.02, Yauco Mun...</td>\n",
       "      <td>615</td>\n",
       "      <td>238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242335</th>\n",
       "      <td>1500000US721537506022</td>\n",
       "      <td>Block Group 2, Census Tract 7506.02, Yauco Mun...</td>\n",
       "      <td>268</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>242336 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       GEO_ID  \\\n",
       "0                   Geography   \n",
       "1       1500000US010010201001   \n",
       "2       1500000US010010201002   \n",
       "3       1500000US010010202001   \n",
       "4       1500000US010010202002   \n",
       "...                       ...   \n",
       "242331  1500000US721537506011   \n",
       "242332  1500000US721537506012   \n",
       "242333  1500000US721537506013   \n",
       "242334  1500000US721537506021   \n",
       "242335  1500000US721537506022   \n",
       "\n",
       "                                                     NAME       B28002_001E  \\\n",
       "0                                    Geographic Area Name  Estimate!!Total:   \n",
       "1       Block Group 1, Census Tract 201, Autauga Count...               252   \n",
       "2       Block Group 2, Census Tract 201, Autauga Count...               357   \n",
       "3       Block Group 1, Census Tract 202, Autauga Count...               272   \n",
       "4       Block Group 2, Census Tract 202, Autauga Count...               328   \n",
       "...                                                   ...               ...   \n",
       "242331  Block Group 1, Census Tract 7506.01, Yauco Mun...               509   \n",
       "242332  Block Group 2, Census Tract 7506.01, Yauco Mun...               577   \n",
       "242333  Block Group 3, Census Tract 7506.01, Yauco Mun...               387   \n",
       "242334  Block Group 1, Census Tract 7506.02, Yauco Mun...               615   \n",
       "242335  Block Group 2, Census Tract 7506.02, Yauco Mun...               268   \n",
       "\n",
       "                                              B28002_004E  \n",
       "0       Estimate!!Total:!!With an Internet subscriptio...  \n",
       "1                                                     200  \n",
       "2                                                     312  \n",
       "3                                                     228  \n",
       "4                                                     242  \n",
       "...                                                   ...  \n",
       "242331                                                333  \n",
       "242332                                                416  \n",
       "242333                                                237  \n",
       "242334                                                238  \n",
       "242335                                                171  \n",
       "\n",
       "[242336 rows x 4 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_avl_df[['GEO_ID', 'NAME', 'B28002_001E', \"B28002_004E\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dc8716-2c13-4b4c-a1f7-dbff58dc4685",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:broadband]",
   "language": "python",
   "name": "conda-env-broadband-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
