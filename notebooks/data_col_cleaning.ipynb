{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93a60c32-3041-4583-8428-e4dc480de4b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os\n",
    "from io import StringIO\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from multiprocessing import Process, current_process, Pool\n",
    "import gc\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "194a8fb7-ec1d-4537-9189-4796e38c3bf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# configure data path\n",
    "current_path = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "data_path = os.path.join(current_path, \"..\", \"data\")\n",
    "\n",
    "data_path = os.path.normpath(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66370d40-66f2-4bbb-b342-fea6e1db9a6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scrape_pages(loc, link):\n",
    "    download_loc = os.path.join(data_path, loc)\n",
    "\n",
    "    # check if location exists\n",
    "    if not os.path.exists(download_loc):\n",
    "        # if the directory does not exist, create it\n",
    "        os.makedirs(download_loc)\n",
    "\n",
    "    # configure selenium and download path\n",
    "    options = Options()\n",
    "    options.set_preference(\"browser.download.folderList\", 2)\n",
    "    options.set_preference(\"browser.download.manager.showWhenStarting\", False)\n",
    "    options.set_preference(\"browser.download.dir\", download_loc)\n",
    "\n",
    "    # get the state page with requests\n",
    "    html_content = requests.get(link)\n",
    "\n",
    "    # beautify the page\n",
    "    soup = BeautifulSoup(html_content.text, \"html.parser\")\n",
    "\n",
    "    # get tables in the hrml page with links to state broadband data\n",
    "    tables = soup.find_all(\"table\")[1]\n",
    "    links = tables.find_all(\"a\")\n",
    "\n",
    "    # loop through the links and download the files using selenium\n",
    "    for link in links:\n",
    "        href = link.get(\"href\")\n",
    "\n",
    "        download_filename = href.split(\"/\")[-1]\n",
    "        download_path = os.path.join(download_loc, download_filename)\n",
    "        temp_download_path = download_path + \".part\"\n",
    "\n",
    "        # check if download files exist\n",
    "        if os.path.isfile(download_path):\n",
    "            print(f\"{download_filename} exist\")\n",
    "        else:\n",
    "            # selenium driver\n",
    "            driver = webdriver.Firefox(options=options)\n",
    "            driver.set_page_load_timeout(5)\n",
    "            try:\n",
    "                driver.get(href)\n",
    "            except TimeoutException:\n",
    "                print(\"Page load timed out but continuing execution.\")\n",
    "\n",
    "            # wait for the download to complete, checking every second\n",
    "            while True:\n",
    "                files = os.listdir(download_loc)\n",
    "                # Check if the .part file for the current download exists\n",
    "                if any(\n",
    "                    file.startswith(download_filename.split(\".\")[0])\n",
    "                    and file.endswith(\".part\")\n",
    "                    for file in files\n",
    "                ):\n",
    "                    print(f\"Download in progress for {download_filename}...\")\n",
    "                    time.sleep(1)\n",
    "                else:\n",
    "                    # If there's no .part file, the download is complete\n",
    "                    print(f\"Download completed for {download_filename}.\")\n",
    "                    break\n",
    "\n",
    "            driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb5aa835-a0ea-47d6-8264-3312a55ac219",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AK-Fixed-Dec2018.zip exist\n",
      "AL-Fixed-Dec2018.zip exist\n",
      "AR-Fixed-Dec2018.zip exist\n",
      "AS-Fixed-Dec2018.zip exist\n",
      "AZ-Fixed-Dec2018.zip exist\n",
      "CA-Fixed-Dec2018.zip exist\n",
      "CO-Fixed-Dec2018.zip exist\n",
      "CT-Fixed-Dec2018.zip exist\n",
      "DC-Fixed-Dec2018.zip exist\n",
      "DE-Fixed-Dec2018.zip exist\n",
      "FL-Fixed-Dec2018.zip exist\n",
      "GA-Fixed-Dec2018.zip exist\n",
      "GU-Fixed-Dec2018.zip exist\n",
      "HI-Fixed-Dec2018.zip exist\n",
      "IA-Fixed-Dec2018.zip exist\n",
      "AK-Fixed-Dec2017.zip exist\n",
      "ID-Fixed-Dec2018.zip existAL-Fixed-Dec2017.zip exist\n",
      "\n",
      "IL-Fixed-Dec2018.zip exist\n",
      "IN-Fixed-Dec2018.zip existAR-Fixed-Dec2017.zip exist\n",
      "KS-Fixed-Dec2018.zip exist\n",
      "\n",
      "AS-Fixed-Dec2017.zip exist\n",
      "KY-Fixed-Dec2018.zip existAZ-Fixed-Dec2017.zip exist\n",
      "\n",
      "CA-Fixed-Dec2017.zip existLA-Fixed-Dec2018.zip exist\n",
      "\n",
      "CO-Fixed-Dec2017.zip exist\n",
      "MA-Fixed-Dec2018.zip existCT-Fixed-Dec2017.zip exist\n",
      "\n",
      "DC-Fixed-Dec2017.zip existMD-Fixed-Dec2018.zip exist\n",
      "DE-Fixed-Dec2017.zip exist\n",
      "\n",
      "FL-Fixed-Dec2017.zip existME-Fixed-Dec2018.zip exist\n",
      "\n",
      "GA-Fixed-Dec2017.zip existMI-Fixed-Dec2018.zip exist\n",
      "\n",
      "GU-Fixed-Dec2017.zip exist\n",
      "MN-Fixed-Dec2018.zip existHI-Fixed-Dec2017.zip exist\n",
      "\n",
      "MO-Fixed-Dec2018.zip existIA-Fixed-Dec2017.zip exist\n",
      "\n",
      "MP-Fixed-Dec2018.zip existID-Fixed-Dec2017.zip exist\n",
      "\n",
      "MS-Fixed-Dec2018.zip exist\n",
      "MT-Fixed-Dec2018.zip existIL-Fixed-Dec2017.zip exist\n",
      "\n",
      "NC-Fixed-Dec2018.zip exist\n",
      "IN-Fixed-Dec2017.zip existND-Fixed-Dec2018.zip exist\n",
      "\n",
      "KS-Fixed-Dec2017.zip existNE-Fixed-Dec2018.zip exist\n",
      "\n",
      "KY-Fixed-Dec2017.zip existNH-Fixed-Dec2018.zip exist\n",
      "\n",
      "LA-Fixed-Dec2017.zip existNJ-Fixed-Dec2018.zip exist\n",
      "\n",
      "MA-Fixed-Dec2017.zip existNM-Fixed-Dec2018.zip exist\n",
      "\n",
      "MD-Fixed-Dec2017.zip existNV-Fixed-Dec2018.zip exist\n",
      "\n",
      "NY-Fixed-Dec2018.zip existME-Fixed-Dec2017.zip exist\n",
      "\n",
      "OH-Fixed-Dec2018.zip exist\n",
      "MI-Fixed-Dec2017.zip existOK-Fixed-Dec2018.zip exist\n",
      "\n",
      "OR-Fixed-Dec2018.zip existMN-Fixed-Dec2017.zip exist\n",
      "\n",
      "PA-Fixed-Dec2018.zip existMO-Fixed-Dec2017.zip exist\n",
      "\n",
      "PR-Fixed-Dec2018.zip existMP-Fixed-Dec2017.zip exist\n",
      "\n",
      "MS-Fixed-Dec2017.zip existRI-Fixed-Dec2018.zip exist\n",
      "MT-Fixed-Dec2017.zip exist\n",
      "\n",
      "NC-Fixed-Dec2017.zip existSC-Fixed-Dec2018.zip exist\n",
      "\n",
      "ND-Fixed-Dec2017.zip exist\n",
      "SD-Fixed-Dec2018.zip existNE-Fixed-Dec2017.zip exist\n",
      "\n",
      "TN-Fixed-Dec2018.zip exist\n",
      "NH-Fixed-Dec2017.zip existTX-Fixed-Dec2018.zip exist\n",
      "\n",
      "NJ-Fixed-Dec2017.zip existUT-Fixed-Dec2018.zip exist\n",
      "\n",
      "NM-Fixed-Dec2017.zip existVA-Fixed-Dec2018.zip exist\n",
      "\n",
      "NV-Fixed-Dec2017.zip existVI-Fixed-Dec2018.zip exist\n",
      "\n",
      "NY-Fixed-Dec2017.zip existVT-Fixed-Dec2018.zip exist\n",
      "\n",
      "OH-Fixed-Dec2017.zip existWA-Fixed-Dec2018.zip exist\n",
      "\n",
      "OK-Fixed-Dec2017.zip exist\n",
      "OR-Fixed-Dec2017.zip existWI-Fixed-Dec2018.zip exist\n",
      "\n",
      "PA-Fixed-Dec2017.zip existWV-Fixed-Dec2018.zip exist\n",
      "\n",
      "WY-Fixed-Dec2018.zip existPR-Fixed-Dec2017.zip exist\n",
      "\n",
      "RI-Fixed-Dec2017.zip exist\n",
      "SC-Fixed-Dec2017.zip exist\n",
      "SD-Fixed-Dec2017.zip exist\n",
      "TN-Fixed-Dec2017.zip exist\n",
      "TX-Fixed-Dec2017.zip exist\n",
      "UT-Fixed-Dec2017.zip exist\n",
      "VA-Fixed-Dec2017.zip exist\n",
      "VI-Fixed-Dec2017.zip exist\n",
      "VT-Fixed-Dec2017.zip exist\n",
      "WA-Fixed-Dec2017.zip exist\n",
      "WI-Fixed-Dec2017.zip exist\n",
      "WV-Fixed-Dec2017.zip exist\n",
      "WY-Fixed-Dec2017.zip exist\n"
     ]
    }
   ],
   "source": [
    "# using multiprocessor\n",
    "def scrape_wrapper(args):\n",
    "    return scrape_pages(*args)\n",
    "\n",
    "\n",
    "# List of arguments for each dataset\n",
    "datasets = [\n",
    "    (\n",
    "        \"2017\",\n",
    "        \"https://www.fcc.gov/form-477-broadband-deployment-data-december-2017-version-3\",\n",
    "    ),\n",
    "    (\n",
    "        \"2018\",\n",
    "        \"https://www.fcc.gov/form-477-broadband-deployment-data-december-2018-version-3\",\n",
    "    ),\n",
    "    # (\"2020\", \"https://www.fcc.gov/form-477-broadband-deployment-data-december-2020\"),\n",
    "]\n",
    "\n",
    "# Using Pool to create a pool of worker processes\n",
    "with Pool(processes=len(datasets)) as pool:\n",
    "    # map the datasets to the scrape_wrapper function\n",
    "    pool.map(scrape_wrapper, datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a208401-5e31-4e93-be00-57a78270c576",
   "metadata": {},
   "source": [
    "### Download 2020 and 2010 census block relationship files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1c260b0-f1e3-4e3f-9263-45ed846ddba6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# download the 2020 2010 block codes concordances\n",
    "# get the state page with requests\n",
    "html_content = requests.get(\n",
    "    \"https://www.census.gov/geographies/reference-files/time-series/geo/relationship-files.2020.html#blkgrp\"\n",
    ")\n",
    "\n",
    "# beautify the page\n",
    "soup = BeautifulSoup(html_content.text, \"html.parser\")\n",
    "\n",
    "# # get tables in the hrml page with links to state broadband data\n",
    "# tables = soup.find_all('table')[1]\n",
    "# links = tables.find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c9c453d-a25e-49e0-bbb6-ee37f62bdb91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TAB2010_TAB2020_ST04.zip exists\n",
      "TAB2010_TAB2020_ST02.zip exists\n",
      "TAB2010_TAB2020_ST01.zip exists\n",
      "TAB2010_TAB2020_ST05.zip exists\n",
      "TAB2010_TAB2020_ST08.zip exists\n",
      "TAB2010_TAB2020_ST06.zip exists\n",
      "TAB2010_TAB2020_ST11.zip exists\n",
      "TAB2010_TAB2020_ST10.zip exists\n",
      "TAB2010_TAB2020_ST09.zip exists\n",
      "TAB2010_TAB2020_ST12.zip exists\n",
      "TAB2010_TAB2020_ST15.zip exists\n",
      "TAB2010_TAB2020_ST13.zip exists\n",
      "TAB2010_TAB2020_ST16.zip exists\n",
      "TAB2010_TAB2020_ST21.zip exists\n",
      "TAB2010_TAB2020_ST17.zip exists\n",
      "TAB2010_TAB2020_ST18.zip exists\n",
      "TAB2010_TAB2020_ST22.zip exists\n",
      "TAB2010_TAB2020_ST20.zip exists\n",
      "TAB2010_TAB2020_ST23.zip exists\n",
      "TAB2010_TAB2020_ST26.zip exists\n",
      "TAB2010_TAB2020_ST25.zip exists\n",
      "TAB2010_TAB2020_ST28.zip exists\n",
      "TAB2010_TAB2020_ST29.zip exists\n",
      "TAB2010_TAB2020_ST30.zip exists\n",
      "TAB2010_TAB2020_ST33.zip exists\n",
      "TAB2010_TAB2020_ST24.zip exists\n",
      "TAB2010_TAB2020_ST27.zip exists\n",
      "TAB2010_TAB2020_ST36.zip exists\n",
      "TAB2010_TAB2020_ST37.zip exists\n",
      "TAB2010_TAB2020_ST31.zip exists\n",
      "TAB2010_TAB2020_ST32.zip exists\n",
      "TAB2010_TAB2020_ST38.zip exists\n",
      "TAB2010_TAB2020_ST35.zip exists\n",
      "TAB2010_TAB2020_ST19.zip exists\n",
      "TAB2010_TAB2020_ST34.zip exists\n",
      "TAB2010_TAB2020_ST44.zip exists\n",
      "TAB2010_TAB2020_ST40.zip exists\n",
      "TAB2010_TAB2020_ST41.zip exists\n",
      "TAB2010_TAB2020_ST42.zip exists\n",
      "TAB2010_TAB2020_ST39.zip exists\n",
      "TAB2010_TAB2020_ST45.zip exists\n",
      "TAB2010_TAB2020_ST47.zip exists\n",
      "TAB2010_TAB2020_ST51.zip exists\n",
      "TAB2010_TAB2020_ST50.zip exists\n",
      "TAB2010_TAB2020_ST46.zip exists\n",
      "TAB2010_TAB2020_ST49.zip exists\n",
      "TAB2010_TAB2020_ST48.zip exists\n",
      "TAB2010_TAB2020_ST53.zip exists\n",
      "TAB2010_TAB2020_ST55.zip exists\n",
      "TAB2010_TAB2020_ST54.zip exists\n",
      "TAB2010_TAB2020_ST56.zip exists\n",
      "TAB2010_TAB2020_ST72.zip exists\n"
     ]
    }
   ],
   "source": [
    "# Find the 'ul' with a specific id\n",
    "div = soup.find(\"div\", {\"id\": \"data-uscb-state-list-selector\"})\n",
    "\n",
    "# # Extract all 'a' tags within the 'ul'\n",
    "links = div.find_all(\"a\") if div else []\n",
    "\n",
    "# Extract the href attribute from each link\n",
    "hrefs = [link.get(\"href\") for link in links]\n",
    "\n",
    "\n",
    "def download_file(href):\n",
    "    download_loc = os.path.join(data_path, \"Blocks20To10\")\n",
    "\n",
    "    # check if location exists\n",
    "    if not os.path.exists(download_loc):\n",
    "        # if the directory does not exist, create it\n",
    "        os.makedirs(download_loc)\n",
    "\n",
    "    # configure selenium and download path\n",
    "    options = Options()\n",
    "    options.set_preference(\"browser.download.folderList\", 2)\n",
    "    options.set_preference(\"browser.download.manager.showWhenStarting\", False)\n",
    "    options.set_preference(\"browser.download.dir\", download_loc)\n",
    "\n",
    "    download_filename = href.split(\"/\")[-1]\n",
    "    download_path = os.path.join(download_loc, download_filename)\n",
    "    temp_download_path = download_path + \".part\"\n",
    "\n",
    "    # Check if the download file already exists\n",
    "    if os.path.isfile(download_path):\n",
    "        print(f\"{download_filename} exists\")\n",
    "        return\n",
    "\n",
    "    # Start the Selenium driver\n",
    "    driver = webdriver.Firefox(options=options)\n",
    "    driver.set_page_load_timeout(5)\n",
    "\n",
    "    try:\n",
    "        driver.get(href)\n",
    "    except TimeoutException:\n",
    "        print(\"Page load timed out but continuing execution.\")\n",
    "\n",
    "    # Wait for the download to complete, checking every second\n",
    "    while True:\n",
    "        files = os.listdir(download_loc)\n",
    "        # Check if the .part file for the current download exists\n",
    "        if any(\n",
    "            file.startswith(download_filename.split(\".\")[0]) and file.endswith(\".part\")\n",
    "            for file in files\n",
    "        ):\n",
    "            print(f\"Download in progress for {download_filename}...\")\n",
    "            time.sleep(1)\n",
    "        else:\n",
    "            # If there's no .part file, the download is complete\n",
    "            print(f\"Download completed for {download_filename}.\")\n",
    "            break\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "\n",
    "# Use ThreadPoolExecutor to download files simultaneously\n",
    "with ThreadPoolExecutor(\n",
    "    max_workers=8\n",
    ") as executor:  # Adjust the number of workers as needed\n",
    "    executor.map(download_file, hrefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e91c9f-77dc-4f55-92ca-49848ca165f9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Map the FCC 2010 Blockcodes to 2020 Block Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d121c3e0-460a-496a-8cd6-6eec7f3504b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# break the 2020, 2021, and 2019 large fcc deployment datasets\n",
    "chunk_size = 200000\n",
    "\n",
    "# latest technologies\n",
    "latest_tech_codes = [43, 50]\n",
    "\n",
    "def break_csv(year, file_name):\n",
    "    csv_path = os.path.join(data_path, file_name)\n",
    "    file_path = os.path.join(data_path, year)\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        os.makedirs(file_path)\n",
    "\n",
    "    # use an iterator to read in chunks\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(\n",
    "            os.path.join(data_path, csv_path),\n",
    "            chunksize=chunk_size,\n",
    "            encoding=\"utf8\",\n",
    "            encoding_errors=\"ignore\",\n",
    "        )\n",
    "    ):\n",
    "        # Process each chunk\n",
    "        for state_abbr, group_df in chunk.groupby(\"StateAbbr\"):\n",
    "            # Define the filename for each state's CSV\n",
    "            filename = f\"{state_abbr}_{year}.csv\"\n",
    "\n",
    "            # latest tech\n",
    "            group_df[\"IsLatestTech\"] = group_df[\"TechCode\"].isin(latest_tech_codes)\n",
    "\n",
    "            # consumer dataframes\n",
    "            consumer_df = group_df[group_df[\"Consumer\"] == 1]\n",
    "\n",
    "            if consumer_df.shape[0] == 0:\n",
    "                continue\n",
    "    \n",
    "            # Append data to the CSV if it already exists, else create a new one\n",
    "            with open(os.path.join(file_path, filename), \"a\") as f:\n",
    "                consumer_df.to_csv(f, index=False, header=f.tell() == 0)\n",
    "                \n",
    "def multiprocessor_wrapper(args):\n",
    "    return break_csv(*args)\n",
    "\n",
    "\n",
    "data = [\n",
    "    (\"2019\", \"fbd_us_without_satellite_dec2019_v1.csv\"),\n",
    "    (\"2020\", \"fbd_us_without_satellite_dec2020_v1.csv\"),\n",
    "    (\"2021\", \"fbd_us_without_satellite_dec2021_v1.csv\"),\n",
    "]\n",
    "\n",
    "with Pool(processes=len(data)) as pool:\n",
    "    # map the datasets to the scrape_wrapper function\n",
    "    pool.map(multiprocessor_wrapper, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5cf929d-b4f4-4f63-b71b-3abd38b4b0ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read the files\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "133618ab-c572-4f57-8c95-888ff237a686",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fips to state abbr relationship file\n",
    "fips_st_df = pd.read_csv(\n",
    "    os.path.join(data_path, \"fips_states\", \"us-state-ansi-fips.csv\")\n",
    ")\n",
    "fips_st_df.columns = [col.strip() for col in fips_st_df.columns]\n",
    "fips_st_df.st = fips_st_df.st.apply(lambda x: str(x).zfill(2))\n",
    "fips_st_df.stusps = fips_st_df.stusps.str.strip()\n",
    "\n",
    "# create a mapping dictionary\n",
    "fips_st_dict = dict(\n",
    "    zip(\n",
    "        fips_st_df.stusps,\n",
    "        fips_st_df.st,\n",
    "    )\n",
    ")\n",
    "# fips_st_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dba45d25-6fa9-4b61-b95c-7a6270afe091",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def zipfile_reader(zip_path):\n",
    "    with ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        # extract the CSV file name from the zip file\n",
    "        contained_file = zip_ref.namelist()[0]\n",
    "\n",
    "        # determine the delimiter based on the file extension\n",
    "        _, ext = os.path.splitext(contained_file)\n",
    "        delimiter = \"|\" if ext.lower() == \".txt\" else \",\"\n",
    "\n",
    "        # open the file within the zip\n",
    "        with zip_ref.open(contained_file) as csvfile:\n",
    "            # read the file into pandas with the appropriate delimiter\n",
    "            df = pd.read_csv(csvfile, delimiter=delimiter)\n",
    "    return df\n",
    "\n",
    "\n",
    "def process_18_17(fcc_path, rlnshp_path, year, is_partially_processed=False):\n",
    "    # year in 2019, 2020, 2021\n",
    "    if is_partially_processed:\n",
    "        df = pd.read_csv(fcc_path)\n",
    "\n",
    "    else:\n",
    "        df = zipfile_reader(fcc_path)\n",
    "        \n",
    "        # consumer dataframes\n",
    "        df = df[df[\"Consumer\"] == 1]\n",
    "        \n",
    "    if year == \"2021\":\n",
    "        df.rename(columns={\"BlockCode\": \"BLKCODE2020\"}, inplace=True)\n",
    "        relationship_dict = {}\n",
    "    else:\n",
    "        # Get the relationship df of the 2010 and 2020 blocks\n",
    "        df_2 = zipfile_reader(rlnshp_path)\n",
    "\n",
    "        # concatenation using vectorized string operations\n",
    "        df_2[\"BLKCODE2010\"] = (\n",
    "            df_2.STATE_2010.astype(str).str.zfill(2)\n",
    "            + df_2.COUNTY_2010.astype(str).str.zfill(3)\n",
    "            + df_2.TRACT_2010.astype(str).str.zfill(6)\n",
    "            + df_2.BLK_2010.astype(str).str.zfill(4)\n",
    "        )\n",
    "\n",
    "        df_2[\"BLKCODE2020\"] = (\n",
    "            df_2.STATE_2020.astype(str).str.zfill(2)\n",
    "            + df_2.COUNTY_2020.astype(str).str.zfill(3)\n",
    "            + df_2.TRACT_2020.astype(str).str.zfill(6)\n",
    "            + df_2.BLK_2020.astype(str).str.zfill(4)\n",
    "        )\n",
    "\n",
    "        df_2[[\"BLKCODE2020\", \"BLKCODE2010\"]] = df_2[[\"BLKCODE2020\", \"BLKCODE2010\"]].astype(\n",
    "            int\n",
    "        )\n",
    "\n",
    "        # Create the relationship dictionary\n",
    "        relationship_dict = dict(zip(df_2.BLKCODE2010, df_2.BLKCODE2020))\n",
    "\n",
    "        # Filter the DataFrame to include only rows where BlockCode is a key in relationship_dict\n",
    "        df = df[df.BlockCode.isin(relationship_dict)]\n",
    "\n",
    "        # Map BlockCode to BLKCODE2020 using the relationship_dict\n",
    "        df[\"BLKCODE2020\"] = df.BlockCode.map(relationship_dict)\n",
    "    \n",
    "    # latest tech\n",
    "    df[\"IsLatestTech\"] = df[\"TechCode\"].isin(latest_tech_codes)\n",
    "\n",
    "    # group by 'BlockCode' and calculate the median 'MaxAdDown' and 'MaxAdUp'\n",
    "    block_stats = (\n",
    "    df.groupby(\"BLKCODE2020\")[[\"MaxAdDown\", \"MaxAdUp\"]]\n",
    "    .agg([\"mean\", \"median\"])\n",
    "    .reset_index()\n",
    "    )\n",
    "\n",
    "    # Renaming the columns\n",
    "    block_stats.columns = ['BLKCODE2020', 'MaxAdDownMean'+ year, 'MaxAdDownMedian'+ year, 'MaxAdUpMean'+ year, 'MaxAdUpMedian'+ year]\n",
    "\n",
    "    # calculate tech ratio\n",
    "    tech_ratio = (\n",
    "        df.groupby(\"BLKCODE2020\")[\"IsLatestTech\"]\n",
    "        .mean()\n",
    "        .reset_index(name=\"LTRatio\")\n",
    "    )\n",
    "\n",
    "    df = block_stats.merge(tech_ratio, on=\"BLKCODE2020\")\n",
    "\n",
    "    df.rename(\n",
    "        columns={\n",
    "            \"LTRatio\": \"LTRatio\" + year,\n",
    "            \"BLKCODE2020\": \"GEOID20\"\n",
    "        },\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    return df, relationship_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc96fc27-e64a-4202-9d6d-dd9cd54fbc99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use the multiprocessing and threading to speed file executions\n",
    "def process_files_for_year(year, batch_size=2):\n",
    "    files = os.listdir(os.path.join(data_path, year))\n",
    "\n",
    "    partially_proc_year = [\"2019\", \"2020\", \"2021\"]\n",
    "\n",
    "    def process_file(file):\n",
    "        if year in partially_proc_year:\n",
    "            is_partially_processed = True\n",
    "            state_abbr = file.split(\"_\")[0]\n",
    "            file_ending = file.endswith(\".csv\")\n",
    "        else:\n",
    "            is_partially_processed = False\n",
    "            state_abbr = file.split(\"-\")[0]\n",
    "            file_ending = file.endswith(\".zip\")\n",
    "\n",
    "        if state_abbr in list(fips_st_dict.keys()) and file_ending:\n",
    "            # Path to state fcc deployment data\n",
    "            fcc_path = os.path.join(data_path, year, file)\n",
    "            file_fips = fips_st_dict[state_abbr]\n",
    "\n",
    "            # Relationship file path\n",
    "            rlnshp_name = \"TAB2010_TAB2020_ST\" + file_fips + \".zip\"\n",
    "            rlnshp_path = os.path.join(data_path, \"Blocks20To10\", rlnshp_name)\n",
    "\n",
    "            df, relationship_dict = process_18_17(\n",
    "                fcc_path, rlnshp_path, year, is_partially_processed\n",
    "            )\n",
    "\n",
    "            # path to save the file\n",
    "            save_path = os.path.join(data_path, year + \"_processed\")\n",
    "\n",
    "            if not os.path.exists(save_path):\n",
    "                os.makedirs(save_path)\n",
    "\n",
    "            output_path = os.path.join(save_path, state_abbr + \"_\" + year + \".csv\")\n",
    "\n",
    "            if not os.path.exists(output_path):\n",
    "                # save the file for future use\n",
    "                df.to_csv(output_path)\n",
    "\n",
    "    # process files in batches\n",
    "    for i in range(0, len(files), batch_size):\n",
    "        batch_files = files[i : i + batch_size]\n",
    "        print(batch_size)\n",
    "        with ThreadPoolExecutor(max_workers=batch_size) as executor:\n",
    "            future_to_file = {\n",
    "                executor.submit(process_file, file): file for file in batch_files\n",
    "            }\n",
    "            for future in as_completed(future_to_file):\n",
    "                file = future_to_file[future]\n",
    "                try:\n",
    "                    data = future.result()\n",
    "                except Exception as exc:\n",
    "                    print(f\"{file} generated an exception: {exc}\")\n",
    "                else:\n",
    "                    print(type(data))\n",
    "                    # print(f'{file} is {len(data)} bytes')\n",
    "                # Explicitly handle garbage collection\n",
    "                gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7611ae90-342f-4608-9e40-a4570b3236e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "years = [\"2017\", \"2018\", \"2019\", \"2020\", \"2021\"]\n",
    "processes = []\n",
    "max_processes = len(years)  # adjust as needed\n",
    "\n",
    "for year in years:\n",
    "    if len(processes) >= max_processes:\n",
    "        for proc in processes:\n",
    "            proc.join()\n",
    "            processes.remove(proc)\n",
    "            gc.collect()  # collect garbage to free memory\n",
    "    process = Process(target=process_files_for_year, args=(year,))\n",
    "    processes.append(process)\n",
    "    process.start()\n",
    "\n",
    "# wait for all processes to finish\n",
    "for process in processes:\n",
    "    process.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1055a3c-68e6-4468-b4a3-2d48658d261d",
   "metadata": {},
   "source": [
    "### Download census track shapefiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6ba5c03-d345-49a1-9b95-9b65c188fe07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.service import Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfb8204a-8310-4889-bd42-5cbbc8b15ad1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_census_tract(val, download_loc, ct_page, fips_id, PAGE_LOAD_TIMEOUT=20):\n",
    "    starts_with = \"tl_2020_\"\n",
    "    filename = \"tl_2020_\" + val + \"_tract.zip\" if fips_id == \"fips_35\" else \"tl_2020_\" + val + \"_bg.zip\" if fips_id == \"fips_34\" else \"tl_2020_\" + val + \"_tabblock20.zip\"\n",
    "\n",
    "    if filename in os.listdir(download_loc):\n",
    "        print(f\"File {filename} already exists. Skipping download.\")\n",
    "        return\n",
    "\n",
    "    options = webdriver.FirefoxOptions()\n",
    "    options.set_preference(\"browser.download.folderList\", 2)\n",
    "    options.set_preference(\"browser.download.manager.showWhenStarting\", False)\n",
    "    options.set_preference(\"browser.download.dir\", download_loc)\n",
    "\n",
    "    local_driver = webdriver.Firefox(options=options)\n",
    "    local_driver.set_page_load_timeout(PAGE_LOAD_TIMEOUT)\n",
    "\n",
    "    try:\n",
    "        local_driver.get(ct_page)\n",
    "        select_element = Select(local_driver.find_element(By.ID, fips_id))\n",
    "        select_element.select_by_value(val)\n",
    "\n",
    "        # Modified to use XPath for button selection\n",
    "        xpath_expression = \"//input[@type='button' and @onclick=\\\"javascript:goDownloadState('2020', 'TABBLOCK20', 'tabblock20', '33', '2020');\\\"]\"\n",
    "        download_button = local_driver.find_element(By.XPATH, xpath_expression)\n",
    "        download_button.click()\n",
    "\n",
    "        # Wait for the download to start and finish\n",
    "        time.sleep(3)  # initial delay for the download to start\n",
    "        while any(file.startswith(starts_with + val) and file.endswith(\".part\") for file in os.listdir(download_loc)):\n",
    "            time.sleep(1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred with value {val}: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        local_driver.quit()\n",
    "\n",
    "def download_census_files(loc, ct_page, fips_id):\n",
    "    download_loc = os.path.join(data_path, loc)\n",
    "    PAGE_LOAD_TIMEOUT = 20\n",
    "\n",
    "    if not os.path.exists(download_loc):\n",
    "        os.makedirs(download_loc)\n",
    "\n",
    "    driver = webdriver.Firefox()\n",
    "    driver.set_page_load_timeout(PAGE_LOAD_TIMEOUT)\n",
    "    driver.get(ct_page)\n",
    "\n",
    "    try:\n",
    "        select_element = Select(driver.find_element(By.ID, fips_id))\n",
    "        values = [option.get_attribute(\"value\") for option in select_element.options if option.get_attribute(\"value\")]\n",
    "\n",
    "        driver.quit()\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "            futures = [executor.submit(download_census_tract, val, download_loc, ct_page, fips_id, PAGE_LOAD_TIMEOUT) for val in values]\n",
    "\n",
    "            for future in as_completed(futures):\n",
    "                try:\n",
    "                    future.result()\n",
    "                except Exception as e:\n",
    "                    print(f\"Exception occurred: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ee1ee05-8633-44d6-897a-365b34e9f88c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loc = \"CensusTractShp\"\n",
    "ct_page = \"https://www.census.gov/cgi-bin/geo/shapefiles/index.php?year=2020&layergroup=Census+Tracts\"\n",
    "fips_id = \"fips_35\"\n",
    "\n",
    "# download census tract shapefiles\n",
    "# download_census_files(loc, ct_page, fips_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69b93a3-d5f0-4c91-af84-67ca7907587e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loc = \"CensusBlockShp\"\n",
    "ct_page = \"https://www.census.gov/cgi-bin/geo/shapefiles/index.php?year=2020&layergroup=Blocks+%282020%29\"\n",
    "fips_id = \"fips_33\"\n",
    "\n",
    "# download census tract shapefiles\n",
    "download_census_files(loc, ct_page, fips_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "908eb211-26f3-417d-b79d-e4a3d219d3cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File tl_2020_01_bg.zip already exists. Skipping download.\n",
      "File tl_2020_02_bg.zip already exists. Skipping download.\n",
      "File tl_2020_04_bg.zip already exists. Skipping download.\n",
      "File tl_2020_60_bg.zip already exists. Skipping download.\n",
      "File tl_2020_05_bg.zip already exists. Skipping download.\n",
      "File tl_2020_06_bg.zip already exists. Skipping download.\n",
      "File tl_2020_08_bg.zip already exists. Skipping download.\n",
      "File tl_2020_09_bg.zip already exists. Skipping download.\n",
      "File tl_2020_69_bg.zip already exists. Skipping download.\n",
      "File tl_2020_10_bg.zip already exists. Skipping download.\n",
      "File tl_2020_12_bg.zip already exists. Skipping download.\n",
      "File tl_2020_11_bg.zip already exists. Skipping download.\n",
      "File tl_2020_13_bg.zip already exists. Skipping download.\n",
      "File tl_2020_66_bg.zip already exists. Skipping download.\n",
      "File tl_2020_16_bg.zip already exists. Skipping download.\n",
      "File tl_2020_15_bg.zip already exists. Skipping download.\n",
      "File tl_2020_18_bg.zip already exists. Skipping download.\n",
      "File tl_2020_17_bg.zip already exists. Skipping download.\n",
      "File tl_2020_19_bg.zip already exists. Skipping download.\n",
      "File tl_2020_20_bg.zip already exists. Skipping download.\n",
      "File tl_2020_21_bg.zip already exists. Skipping download.\n",
      "File tl_2020_23_bg.zip already exists. Skipping download.\n",
      "File tl_2020_22_bg.zip already exists. Skipping download.\n",
      "File tl_2020_25_bg.zip already exists. Skipping download.\n",
      "File tl_2020_24_bg.zip already exists. Skipping download.\n",
      "File tl_2020_26_bg.zip already exists. Skipping download.\n",
      "File tl_2020_27_bg.zip already exists. Skipping download.\n",
      "File tl_2020_28_bg.zip already exists. Skipping download.\n",
      "File tl_2020_29_bg.zip already exists. Skipping download.\n",
      "File tl_2020_30_bg.zip already exists. Skipping download.\n",
      "File tl_2020_31_bg.zip already exists. Skipping download.\n",
      "File tl_2020_33_bg.zip already exists. Skipping download.\n",
      "File tl_2020_32_bg.zip already exists. Skipping download.\n",
      "File tl_2020_34_bg.zip already exists. Skipping download.\n",
      "File tl_2020_35_bg.zip already exists. Skipping download.\n",
      "File tl_2020_36_bg.zip already exists. Skipping download.\n",
      "File tl_2020_38_bg.zip already exists. Skipping download.\n",
      "File tl_2020_37_bg.zip already exists. Skipping download.\n",
      "File tl_2020_40_bg.zip already exists. Skipping download.\n",
      "File tl_2020_39_bg.zip already exists. Skipping download.\n",
      "File tl_2020_41_bg.zip already exists. Skipping download.\n",
      "File tl_2020_42_bg.zip already exists. Skipping download.\n",
      "File tl_2020_72_bg.zip already exists. Skipping download.\n",
      "File tl_2020_45_bg.zip already exists. Skipping download.\n",
      "File tl_2020_44_bg.zip already exists. Skipping download.\n",
      "File tl_2020_46_bg.zip already exists. Skipping download.\n",
      "File tl_2020_48_bg.zip already exists. Skipping download.\n",
      "File tl_2020_47_bg.zip already exists. Skipping download.\n",
      "File tl_2020_49_bg.zip already exists. Skipping download.\n",
      "File tl_2020_50_bg.zip already exists. Skipping download.\n",
      "File tl_2020_78_bg.zip already exists. Skipping download.\n",
      "File tl_2020_51_bg.zip already exists. Skipping download.\n",
      "File tl_2020_53_bg.zip already exists. Skipping download.\n",
      "File tl_2020_54_bg.zip already exists. Skipping download.\n",
      "File tl_2020_55_bg.zip already exists. Skipping download.\n",
      "File tl_2020_56_bg.zip already exists. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "loc = \"CensusBlockGroups\"\n",
    "ct_page = \"https://www.census.gov/cgi-bin/geo/shapefiles/index.php?year=2020&layergroup=Block+Groups\"\n",
    "fips_id = \"fips_34\"\n",
    "\n",
    "# download census tract shapefiles\n",
    "download_census_files(loc, ct_page, fips_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6bd643-7783-4b13-9340-f4d37ee8c81f",
   "metadata": {},
   "source": [
    "### Clean other demographic factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "109fcd21-7dc8-4885-b533-50581b67c6f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# clean income data, esp with uncertainties such as 25000-, 250000+ and - (nans)\n",
    "def transform_income(value):\n",
    "    if type(value) == str:\n",
    "        # Remove commas\n",
    "        value = value.replace(\",\", \"\")\n",
    "\n",
    "        # Handle different cases\n",
    "        if \"-\" in value:\n",
    "            if value.endswith(\"-\") and len(value) != 1:\n",
    "                # One-sided range, use the provided value\n",
    "                return float(value.replace(\"-\", \"\"))\n",
    "            elif value.startswith(\"-\"):\n",
    "                # Missing or uncertain data\n",
    "                return None  # Or use 0, or a specific strategy for missing data\n",
    "            else:\n",
    "                # Range, take the average\n",
    "                low, high = value.split(\"-\")\n",
    "                return (float(low) + float(high)) / 2\n",
    "        elif \"+\" in value:\n",
    "            # Open-ended value, use the provided number\n",
    "            return float(value.replace(\"+\", \"\"))\n",
    "        elif value == \"-\":\n",
    "            # Missing or uncertain data\n",
    "            return None  # Or use mean/median of the column\n",
    "        else:\n",
    "            # Regular value\n",
    "            return float(value)\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0fd04c9e-b8cc-4c65-9598-1c3041db37f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "income_data_path = os.path.join(\n",
    "    data_path, \"median_income\", \"ACSDT5Y2021.B19049-Data.csv\"\n",
    ")\n",
    "edu_data_path = os.path.join(data_path, \"education\", \"ACSDT5Y2021.B15003-Data.csv\")\n",
    "urban_rural_data_path = os.path.join(\n",
    "    data_path, \"urban_rural\", \"DECENNIALDHC2020.P2-Data.csv\"\n",
    ")\n",
    "internet_avl_data_path = os.path.join(\n",
    "    data_path, \"internet_availability\", \"ACSDT5Y2021.B28002-Data.csv\"\n",
    ")\n",
    "\n",
    "# read the csv files\n",
    "# income data\n",
    "income_df = pd.read_csv(income_data_path, usecols=[\"GEO_ID\", \"NAME\", \"B19049_001E\"])\n",
    "income_df.rename(columns={\"B19049_001E\": \"median_income\"}, inplace=True)\n",
    "income_df.drop(0, inplace=True)\n",
    "income_df[\"median_income\"] = income_df[\"median_income\"].apply(transform_income)\n",
    "income_df[\"median_income\"].fillna(income_df[\"median_income\"].mean(), inplace=True)\n",
    "\n",
    "# education data\n",
    "edu_df = pd.read_csv(\n",
    "    edu_data_path, usecols=[\"GEO_ID\", \"NAME\", \"B15003_001E\", \"B15003_022E\"]\n",
    ")\n",
    "edu_df.drop(0, inplace=True)\n",
    "edu_df[\"with_degree\"] = edu_df[\"B15003_022E\"].astype(int) / edu_df[\n",
    "    \"B15003_001E\"\n",
    "].astype(int)\n",
    "\n",
    "# ratio of rural to urban\n",
    "urban_df = pd.read_csv(\n",
    "    urban_rural_data_path, usecols=[\"GEO_ID\", \"NAME\", \"P2_001N\", \"P2_002N\"]\n",
    ")\n",
    "urban_df.drop(0, inplace=True)\n",
    "urban_df[\"urban_pop\"] = urban_df[\"P2_002N\"].astype(int) / urban_df[\"P2_001N\"].astype(\n",
    "    int\n",
    ")\n",
    "urban_df.rename(columns={\"P2_001N\": \"POP20\"}, inplace=True)\n",
    "\n",
    "# with broadband of any type\n",
    "int_avl_df = pd.read_csv(\n",
    "    internet_avl_data_path, usecols=[\"GEO_ID\", \"NAME\", \"B28002_001E\", \"B28002_004E\"]\n",
    ")\n",
    "int_avl_df.drop(0, inplace=True)\n",
    "int_avl_df[\"with_broadband\"] = int_avl_df[\"B28002_004E\"].astype(int) / int_avl_df[\n",
    "    \"B28002_001E\"\n",
    "].astype(int)\n",
    "\n",
    "# merge the datasets and extract the columns of interest\n",
    "\n",
    "data = (\n",
    "    income_df[[\"GEO_ID\", \"median_income\"]]\n",
    "    .merge(edu_df[[\"GEO_ID\", \"with_degree\"]], on=\"GEO_ID\")\n",
    "    .merge(urban_df[[\"GEO_ID\", \"urban_pop\", \"POP20\"]], on=\"GEO_ID\")\n",
    "    .merge(int_avl_df[[\"GEO_ID\", \"with_broadband\"]], on=\"GEO_ID\")\n",
    ")\n",
    "\n",
    "output_path = os.path.join(data_path, \"in_urb_ed_brb.csv\")\n",
    "\n",
    "# get census track\n",
    "data[\"GEOID\"] = data.GEO_ID.str[9:]\n",
    "data.to_csv(output_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:us-broadband]",
   "language": "python",
   "name": "conda-env-us-broadband-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
